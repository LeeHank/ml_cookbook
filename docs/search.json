[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"自己整理的 ML coding notes","code":""},{"path":"linear-model.html","id":"linear-model","chapter":"1 Linear Model","heading":"1 Linear Model","text":"","code":""},{"path":"glmnet.html","id":"glmnet","chapter":"2 glmnet","heading":"2 glmnet","text":"","code":""},{"path":"glmnet.html","id":"理論","chapter":"2 glmnet","heading":"2.1 理論","text":"","code":""},{"path":"glmnet.html","id":"lasso-實作","chapter":"2 glmnet","heading":"2.2 lasso 實作","text":"這篇文章摘自 Julia Silge 在 2020/5/17 寫的 blog 連結主要目的，是用 lasso regression，去預測 Office 這個美國暢銷影集的某集 IMDB ratings","code":"\nlibrary(tidyverse)"},{"path":"glmnet.html","id":"explore-the-data","chapter":"2 glmnet","heading":"2.2.1 Explore the data","text":"要拿來預測用的資料，n = 135, p = 32變數說明：\nimdb_rating: 要預測的目標 y (連續型)\nepisode_name 是 ID 欄位\nseason 有被我當 predictor，因為我猜不同季的 rating會不同(例如一開始rating還好，到中間口碑變很好所以rating高，到最後幾季又開始拖戲和爛尾所以 rating 低)\nepisode 也有被我當 predictor, 因為我猜不同 episode 的 rating 可能也不同 (例如每季剛開播，跟接近結束，可能 rating 較高，中間 rating 較低)\nandy, angela, …, justin_spitzer 共 28 個變數，都是人名，裡面的數值，表示該演員在這一集裡面，講過多少次話。這些也被我當 predictor，我猜有些演員很討喜，他講越多話 rating 可能越好。\nimdb_rating: 要預測的目標 y (連續型)episode_name 是 ID 欄位season 有被我當 predictor，因為我猜不同季的 rating會不同(例如一開始rating還好，到中間口碑變很好所以rating高，到最後幾季又開始拖戲和爛尾所以 rating 低)episode 也有被我當 predictor, 因為我猜不同 episode 的 rating 可能也不同 (例如每季剛開播，跟接近結束，可能 rating 較高，中間 rating 較低)andy, angela, …, justin_spitzer 共 28 個變數，都是人名，裡面的數值，表示該演員在這一集裡面，講過多少次話。這些也被我當 predictor，我猜有些演員很討喜，他講越多話 rating 可能越好。整體看一下，有沒有 missing，以及分佈的狀況：Table 2.1: Data summaryVariable type: characterVariable type: numeric全部欄位都沒有 missing， y 的分佈蠻不錯的常態這邊再做一個 EDA ，看看 episode 和 rating有沒有關係 (是不是每一季越到後面的集數，rating會越高？)看起來的確有這個趨勢啊！","code":"\n# ratings_raw <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv\")\n# \n# remove_regex = \"[:punct:]|[:digit:]|parts |part |the |and\"\n# \n# office_ratings <- ratings_raw %>%\n#   transmute(\n#     episode_name = str_to_lower(title),\n#     episode_name = str_remove_all(\n#       episode_name, remove_regex\n#     ),\n#     episode_name = str_trim(episode_name),\n#     imdb_rating\n#   )\n# \n# office_info <- schrute::theoffice %>%\n#   mutate(\n#     season = as.numeric(season),\n#     episode = as.numeric(episode),\n#     episode_name = str_to_lower(episode_name),\n#     episode_name = str_remove_all(episode_name, remove_regex),\n#     episode_name = str_trim(episode_name)\n#   ) %>%\n#   select(season, episode, episode_name, director, writer, character)\n# \n# characters <- office_info %>%\n#   count(episode_name, character) %>%\n#   add_count(character, wt = n, name = \"character_count\") %>%\n#   filter(character_count > 800) %>%\n#   select(-character_count) %>%\n#   pivot_wider(\n#     names_from = character,\n#     values_from = n,\n#     values_fill = list(n = 0)\n#   )\n# \n# creators <- office_info %>%\n#   distinct(episode_name, director, writer) %>%\n#   pivot_longer(director:writer, names_to = \"role\", values_to = \"person\") %>%\n#   separate_rows(person, sep = \";\") %>%\n#   add_count(person) %>%\n#   filter(n > 10) %>%\n#   distinct(episode_name, person) %>%\n#   mutate(person_value = 1) %>%\n#   pivot_wider(\n#     names_from = person,\n#     values_from = person_value,\n#     values_fill = list(person_value = 0)\n#   )\n# \n# office <- office_info %>%\n#   distinct(season, episode, episode_name) %>%\n#   inner_join(characters) %>%\n#   inner_join(creators) %>%\n#   inner_join(office_ratings %>%\n#     select(episode_name, imdb_rating)) %>%\n#   janitor::clean_names()\n# \n# saveRDS(office, \"model_example/glmnet/office.rds\")\noffice = readRDS(\"./data/office.rds\")\noffice\n#> # A tibble: 136 × 32\n#>    season episode episode_name     andy angela darryl dwight\n#>     <dbl>   <dbl> <chr>           <int>  <int>  <int>  <int>\n#>  1      1       1 pilot               0      1      0     29\n#>  2      1       2 diversity day       0      4      0     17\n#>  3      1       3 health care         0      5      0     62\n#>  4      1       5 basketball          0      3     15     25\n#>  5      1       6 hot girl            0      3      0     28\n#>  6      2       1 dundies             0      1      1     32\n#>  7      2       2 sexual harassm…     0      2      9     11\n#>  8      2       3 office olympics     0      6      0     55\n#>  9      2       4 fire                0     17      0     65\n#> 10      2       5 halloween           0     13      0     33\n#> # … with 126 more rows, and 25 more variables: jim <int>,\n#> #   kelly <int>, kevin <int>, michael <int>, oscar <int>,\n#> #   pam <int>, phyllis <int>, ryan <int>, toby <int>,\n#> #   erin <int>, jan <int>, ken_kwapis <dbl>,\n#> #   greg_daniels <dbl>, b_j_novak <dbl>,\n#> #   paul_lieberstein <dbl>, mindy_kaling <dbl>,\n#> #   paul_feig <dbl>, gene_stupnitsky <dbl>, …\nskimr::skim(office)\noffice %>%\n  ggplot(aes(episode, imdb_rating, fill = as.factor(episode))) +\n  geom_boxplot(show.legend = FALSE)"},{"path":"glmnet.html","id":"train-a-model","chapter":"2 glmnet","heading":"2.2.2 Train a model","text":"","code":""},{"path":"glmnet.html","id":"split-data","chapter":"2 glmnet","heading":"2.2.2.1 split data","text":"","code":"\nlibrary(rsample)\nset.seed(1234)\noffice_split <- initial_split(office, strata = season)\noffice_train <- training(office_split)\noffice_test <- testing(office_split)\n\n# 資料太少，不用 cv ，改用 bootstrap\nset.seed(1234)\noffice_boot <- bootstraps(office_train, strata = season)"},{"path":"glmnet.html","id":"preprocessing","chapter":"2 glmnet","heading":"2.2.2.2 preprocessing","text":"","code":"\nlibrary(recipes)\noffice_rec <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(episode_name, new_role = \"ID\") %>% # episode 不把他當 predictor\n  step_zv(all_numeric(), -all_outcomes()) %>% # zero variance 對 回歸問題都會造成影響\n  step_normalize(all_numeric(), -all_outcomes()) # lasso 需要做 normalize"},{"path":"glmnet.html","id":"specify-model","chapter":"2 glmnet","heading":"2.2.2.3 specify model","text":"可以看到，訂 model 的時候，我 specify 他的 penalty 要用 tuning 的，然後 mixture設為1 (就會是 lasso)","code":"\nlibrary(parsnip)\nlibrary(tune)\nlasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")"},{"path":"glmnet.html","id":"workflow-setting","chapter":"2 glmnet","heading":"2.2.2.4 workflow setting","text":"","code":"\nlibrary(workflows)\nlasso_wf = workflow() %>%\n  add_recipe(office_rec) %>%\n  add_model(lasso_spec)\n\nlasso_wf\n#> ══ Workflow ════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: linear_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────\n#> 2 Recipe Steps\n#> \n#> • step_zv()\n#> • step_normalize()\n#> \n#> ── Model ───────────────────────────────────────────────────\n#> Linear Regression Model Specification (regression)\n#> \n#> Main Arguments:\n#>   penalty = tune()\n#>   mixture = 1\n#> \n#> Computational engine: glmnet"},{"path":"glmnet.html","id":"hyper-parameter-setting","chapter":"2 glmnet","heading":"2.2.2.5 hyper parameter setting","text":"從 hyper parameter 的 meta data table，可看出：\nidentifier: penalty ，是這個超參數的 id\nobject: nparam[+] 的意思是，他是 numeric parameter，+ 表示已有設定 range 在裡面\nidentifier: penalty ，是這個超參數的 idobject: nparam[+] 的意思是，他是 numeric parameter，+ 表示已有設定 range 在裡面我們可以這樣看到他的 range，是在 log10 的尺度下，從 -10 到 0我想用 latin_hypercube，幫他在 feature space 中均勻撒 50 個點","code":"\nlibrary(dials)\nhyper_param_meta = lasso_spec %>%\n  parameters() %>%\n  finalize(office_train)\n\nhyper_param_meta\n#> Collection of 1 parameters for tuning\n#> \n#>  identifier    type    object\n#>     penalty penalty nparam[+]\nhyper_param_meta %>%\n  pull_dials_object(\"penalty\")\n#> Amount of Regularization (quantitative)\n#> Transformer:  log-10 \n#> Range (transformed scale): [-10, 0]\nmy_grid = grid_latin_hypercube(hyper_param_meta, size = 50)\nmy_grid %>%\n  arrange(penalty)\n#> # A tibble: 50 × 1\n#>     penalty\n#>       <dbl>\n#>  1 1.12e-10\n#>  2 2.20e-10\n#>  3 3.64e-10\n#>  4 4.34e-10\n#>  5 7.01e-10\n#>  6 1.42e- 9\n#>  7 1.98e- 9\n#>  8 3.41e- 9\n#>  9 4.02e- 9\n#> 10 9.39e- 9\n#> # … with 40 more rows"},{"path":"glmnet.html","id":"model-fitting","chapter":"2 glmnet","heading":"2.2.2.6 model fitting","text":"","code":""},{"path":"glmnet.html","id":"tune-hyper-parameter","chapter":"2 glmnet","heading":"2.2.2.6.1 tune hyper parameter","text":"看一下 tune 完後，最佳的 penalty 訂為多少是 0.0481234來看一下 tunning 的過程：nice，可以看到 lasso 的確有幫助到結果 (但看 r-square 可以發現頗慘烈，才 15% 而已)","code":"\n# 開平行運算\nlibrary(doParallel)\ncl <- makePSOCKcluster(4) # Create a cluster object\nregisterDoParallel(cl) # register\n\nlibrary(yardstick)\n# fitting\nset.seed(130)\nlasso_tune <- \n  tune::tune_grid(\n    object = lasso_wf,\n    resamples = office_boot,\n    metrics = metric_set(rmse, rsq),\n    grid = my_grid,\n    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n)\n\n# 關平行運算\nstopCluster(cl)\nfinal_param = lasso_tune %>% select_best(metric = \"rmse\", maximize = FALSE)\nfinal_param\n#> # A tibble: 1 × 2\n#>   penalty .config              \n#>     <dbl> <chr>                \n#> 1  0.0481 Preprocessor1_Model44\nlasso_tune %>%\n  collect_metrics() %>%\n  ggplot(aes(x = penalty, y = mean, color = .metric)) +\n  geom_errorbar(\n    aes(ymin = mean - std_err, ymax = mean + std_err), \n    alpha = 0.5\n  ) +\n  geom_line(size = 1.5) +\n  geom_vline(xintercept = final_param$penalty, color = \"blue\", lty = 2) +\n  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\n  scale_x_log10() +\n  theme_bw() +\n  theme(legend.position = \"none\")"},{"path":"glmnet.html","id":"finalize-workflow-model","chapter":"2 glmnet","heading":"2.2.2.7 finalize workflow & model","text":"最後，用這組最佳參數，去 finalize 我們的 model","code":"\nfinal_lasso_wf = lasso_wf %>% finalize_workflow(final_param) # finalized workflow\nfinal_lasso_fit <- final_lasso_wf %>% fit(office_train) # finalized model"},{"path":"glmnet.html","id":"prediction","chapter":"2 glmnet","heading":"2.2.3 Prediction","text":"對測試集做預測","code":"\n# 對測試集做預測\noffice_test_res <- bind_cols(\n  stats::predict(final_lasso_fit, office_test), # 預測值\n  office_test %>% select(imdb_rating) # 真值\n)\noffice_test_res\n#> # A tibble: 36 × 2\n#>    .pred imdb_rating\n#>    <dbl>       <dbl>\n#>  1  8.56         7.6\n#>  2  8.80         8.2\n#>  3  8.33         8.2\n#>  4  8.67         8.2\n#>  5  8.39         7.9\n#>  6  8.46         8.2\n#>  7  8.41         8.3\n#>  8  8.26         8  \n#>  9  8.00         8.2\n#> 10  8.31         8.5\n#> # … with 26 more rows"},{"path":"glmnet.html","id":"evaluation","chapter":"2 glmnet","heading":"2.2.4 Evaluation","text":"hmm… r-square 實在是差強人意啊～","code":"\noffice_metrics <- yardstick::metric_set(rmse, rsq, mae)\noffice_metrics(\n  data = office_test_res, \n  truth = imdb_rating, \n  estimate = .pred\n)\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard       0.428\n#> 2 rsq     standard       0.109\n#> 3 mae     standard       0.343"},{"path":"glmnet.html","id":"explaination","chapter":"2 glmnet","heading":"2.2.5 Explaination","text":"","code":""},{"path":"glmnet.html","id":"變數重要性","chapter":"2 glmnet","heading":"2.2.5.1 變數重要性","text":"先來看一下我們 fit 出的 model:可以看到，最重要的是michael，再來是greg_daniels, 依此類推通常在處理變數重要性時，都會用 vip 這個 package，因為他還有很多額外功能。可以看到，結果和剛剛的係數一模一樣。那這邊特別提醒，vi() 裡面要下 lambda = 這個 argument。我一開始沒有下這個 argument，他就會自動去取最小的lambda值帶入，那就幾乎所有變數的係數都 >0，根本不是我要的。解釋可參考這裡可以看到，結果和剛剛的係數一模一樣。那這邊特別提醒，vi() 裡面要下 lambda = 這個 argument。我一開始沒有下這個 argument，他就會自動去取最小的lambda值帶入，那就幾乎所有變數的係數都 >0，根本不是我要的。解釋可參考這裡那我們幫這個變數重要性畫圖：那我們幫這個變數重要性畫圖：可以看到，最重要的是michael(收視保證啊～)那如果要挑出變數給別人看，就 filter 出係數大於 0 的變數就好：","code":"\n# parsnip model\nparsnip_model = final_lasso_fit %>%\n  fit(office_train) %>%\n  pull_workflow_fit()\ncoef_df = parsnip_model %>%\n  tidy() %>%\n  arrange(desc(abs(estimate)))\n\ncoef_df\n#> # A tibble: 31 × 3\n#>    term            estimate penalty\n#>    <chr>              <dbl>   <dbl>\n#>  1 (Intercept)       8.37    0.0481\n#>  2 michael           0.162   0.0481\n#>  3 greg_daniels      0.101   0.0481\n#>  4 episode           0.0613  0.0481\n#>  5 kelly            -0.0420  0.0481\n#>  6 jim               0.0410  0.0481\n#>  7 jan               0.0406  0.0481\n#>  8 angela            0.0387  0.0481\n#>  9 paul_feig         0.0332  0.0481\n#> 10 randall_einhorn  -0.0277  0.0481\n#> # … with 21 more rows\nlibrary(vip)\nparsnip_model %>%\n  vip::vi(lambda = final_param$penalty)\n#> # A tibble: 30 × 3\n#>    Variable         Importance Sign \n#>    <chr>                 <dbl> <chr>\n#>  1 michael              0.162  POS  \n#>  2 greg_daniels         0.101  POS  \n#>  3 episode              0.0613 POS  \n#>  4 kelly                0.0420 NEG  \n#>  5 jim                  0.0410 POS  \n#>  6 jan                  0.0406 POS  \n#>  7 angela               0.0387 POS  \n#>  8 paul_feig            0.0332 POS  \n#>  9 randall_einhorn      0.0277 NEG  \n#> 10 paul_lieberstein     0.0119 POS  \n#> # … with 20 more rows\nparsnip_model %>%\n  vip::vi(lambda = final_param$penalty) %>%\n  mutate(\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance)\n  ) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL)\nparsnip_model %>%\n  tidy() %>%\n  filter(estimate!=0) %>%\n  arrange(desc(abs(estimate)))\n#> # A tibble: 13 × 3\n#>    term             estimate penalty\n#>    <chr>               <dbl>   <dbl>\n#>  1 (Intercept)       8.37     0.0481\n#>  2 michael           0.162    0.0481\n#>  3 greg_daniels      0.101    0.0481\n#>  4 episode           0.0613   0.0481\n#>  5 kelly            -0.0420   0.0481\n#>  6 jim               0.0410   0.0481\n#>  7 jan               0.0406   0.0481\n#>  8 angela            0.0387   0.0481\n#>  9 paul_feig         0.0332   0.0481\n#> 10 randall_einhorn  -0.0277   0.0481\n#> 11 paul_lieberstein  0.0119   0.0481\n#> 12 darryl            0.00653  0.0481\n#> 13 dwight            0.00592  0.0481"},{"path":"glmnet.html","id":"統計推論","chapter":"2 glmnet","heading":"2.2.6 統計推論","text":"如果要做統計推論(哪些變數顯著)，","code":""},{"path":"glmnet.html","id":"模型細節","chapter":"2 glmnet","heading":"2.2.6.1 模型細節","text":"如果你想看 glmnet 的模型細節，可以先取出原生 package 的物件：","code":"\n# 原生 package 的物件\nglmnet_model = parsnip_model$fit"},{"path":"glmnet.html","id":"法一用-broom-幫忙-summarise-推薦","chapter":"2 glmnet","heading":"2.2.6.1.1 法一：用 broom 幫忙 summarise (推薦)","text":"可以看到，每一個 term (變數)，在每一個 step (.e. 對應的 lambda 下)，所得到的係數估計值，以及解釋變異量(dev.ratio, fraction null deviance explained value lambda)他的 step 都是 1:72，從最大的 lambda 到最小的 lambda來畫一下 shrinkage 過程","code":"\nglmnet_model %>%\n  broom::tidy(return_zeros = FALSE)\n#> # A tibble: 1,550 × 5\n#>    term         step estimate lambda dev.ratio\n#>    <chr>       <dbl>    <dbl>  <dbl>     <dbl>\n#>  1 (Intercept)     1     8.37  0.242    0     \n#>  2 (Intercept)     2     8.37  0.220    0.0357\n#>  3 (Intercept)     3     8.37  0.201    0.0654\n#>  4 (Intercept)     4     8.37  0.183    0.0900\n#>  5 (Intercept)     5     8.37  0.167    0.110 \n#>  6 (Intercept)     6     8.37  0.152    0.136 \n#>  7 (Intercept)     7     8.37  0.138    0.162 \n#>  8 (Intercept)     8     8.37  0.126    0.183 \n#>  9 (Intercept)     9     8.37  0.115    0.201 \n#> 10 (Intercept)    10     8.37  0.105    0.223 \n#> # … with 1,540 more rows\ntidied <- tidy(glmnet_model) %>% \n  filter(term != \"(Intercept)\")\n\n tidied %>%\n   ggplot(aes(lambda, estimate, col = term)) +\n   geom_line() +\n   scale_x_log10()"},{"path":"glmnet.html","id":"法二原生-pacage-的做法","chapter":"2 glmnet","heading":"2.2.6.1.2 法二：原生 pacage 的做法","text":"這是 glmnet 的標準output：\n第三欄的 Lambda，就是 penalty 為多少\n第一欄的 Df = degree freedom = 參數個數 = 有幾個變數的係數不等於 0\n第二欄的 %Dev 是指 percent deviance explained\n第一列，是 Lambda 最大時 (0.21)，沒有任何一個變數係數大於0。然後 lambda 慢慢放寬後，越來越多變數的係數大於 0\n第三欄的 Lambda，就是 penalty 為多少第一欄的 Df = degree freedom = 參數個數 = 有幾個變數的係數不等於 0第二欄的 %Dev 是指 percent deviance explained第一列，是 Lambda 最大時 (0.21)，沒有任何一個變數係數大於0。然後 lambda 慢慢放寬後，越來越多變數的係數大於 0如果想看各個變數 shrinkage 的過程，快速的方法可以這樣做：如果想畫美美的圖，就去拿原始資料：lambda mapping 自己做：然後，自己轉成畫圖用資料：","code":"\nglmnet_model\n#> \n#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n#> \n#>    Df  %Dev   Lambda\n#> 1   0  0.00 0.241600\n#> 2   1  3.57 0.220200\n#> 3   1  6.54 0.200600\n#> 4   1  9.00 0.182800\n#> 5   1 11.04 0.166500\n#> 6   2 13.60 0.151800\n#> 7   2 16.16 0.138300\n#> 8   2 18.28 0.126000\n#> 9   3 20.09 0.114800\n#> 10  4 22.33 0.104600\n#> 11  6 24.72 0.095310\n#> 12  8 27.41 0.086840\n#> 13 10 30.23 0.079120\n#> 14 10 33.18 0.072090\n#> 15 10 35.63 0.065690\n#> 16 11 37.69 0.059850\n#> 17 11 39.51 0.054540\n#> 18 12 41.20 0.049690\n#> 19 12 42.66 0.045280\n#> 20 12 43.86 0.041260\n#> 21 12 44.86 0.037590\n#> 22 12 45.69 0.034250\n#> 23 12 46.38 0.031210\n#> 24 12 46.96 0.028440\n#> 25 13 47.52 0.025910\n#> 26 16 48.03 0.023610\n#> 27 18 48.54 0.021510\n#> 28 20 49.08 0.019600\n#> 29 20 49.63 0.017860\n#> 30 20 50.08 0.016270\n#> 31 21 50.65 0.014830\n#> 32 21 51.18 0.013510\n#> 33 21 51.62 0.012310\n#> 34 20 51.98 0.011220\n#> 35 22 52.36 0.010220\n#> 36 22 52.71 0.009311\n#> 37 22 53.00 0.008484\n#> 38 22 53.24 0.007730\n#> 39 22 53.43 0.007044\n#> 40 22 53.60 0.006418\n#> 41 23 53.74 0.005848\n#> 42 24 53.88 0.005328\n#> 43 25 54.02 0.004855\n#> 44 25 54.14 0.004424\n#> 45 25 54.23 0.004031\n#> 46 25 54.31 0.003673\n#> 47 27 54.38 0.003346\n#> 48 27 54.45 0.003049\n#> 49 28 54.52 0.002778\n#> 50 28 54.58 0.002531\n#> 51 28 54.62 0.002307\n#> 52 28 54.66 0.002102\n#> 53 28 54.70 0.001915\n#> 54 28 54.73 0.001745\n#> 55 28 54.75 0.001590\n#> 56 28 54.77 0.001449\n#> 57 28 54.78 0.001320\n#> 58 28 54.80 0.001203\n#> 59 28 54.81 0.001096\n#> 60 28 54.82 0.000998\n#> 61 28 54.82 0.000910\n#> 62 28 54.83 0.000829\n#> 63 28 54.84 0.000755\n#> 64 28 54.84 0.000688\n#> 65 28 54.84 0.000627\n#> 66 28 54.85 0.000571\n#> 67 28 54.85 0.000521\n#> 68 28 54.85 0.000474\n#> 69 28 54.85 0.000432\n#> 70 28 54.86 0.000394\n#> 71 28 54.86 0.000359\n#> 72 28 54.86 0.000327\n#> 73 28 54.86 0.000298\n#> 74 28 54.86 0.000271\n#> 75 28 54.86 0.000247\n#> 76 28 54.86 0.000225\nplot(glmnet_model, xvar = \"lambda\", label = TRUE)\nglmnet_model$beta %>%\n  as.matrix() %>%\n  as.data.frame() %>%\n  .[c(1:5),c(1:5)]\n#>         s0 s1 s2 s3 s4\n#> season   0  0  0  0  0\n#> episode  0  0  0  0  0\n#> andy     0  0  0  0  0\n#> angela   0  0  0  0  0\n#> darryl   0  0  0  0  0\nlambda_mapping = data.frame(\n  lambda_index = paste0(\"s\", 0:(length(glmnet_model$lambda)-1)),\n  lambda = glmnet_model$lambda\n)\nlambda_mapping\n#>    lambda_index       lambda\n#> 1            s0 0.2416331546\n#> 2            s1 0.2201671311\n#> 3            s2 0.2006080900\n#> 4            s3 0.1827866202\n#> 5            s4 0.1665483606\n#> 6            s5 0.1517526633\n#> 7            s6 0.1382713749\n#> 8            s7 0.1259877271\n#> 9            s8 0.1147953247\n#> 10           s9 0.1045972244\n#> 11          s10 0.0953050953\n#> 12          s11 0.0868384533\n#> 13          s12 0.0791239644\n#> 14          s13 0.0720948095\n#> 15          s14 0.0656901053\n#> 16          s15 0.0598543774\n#> 17          s16 0.0545370795\n#> 18          s17 0.0496921557\n#> 19          s18 0.0452776415\n#> 20          s19 0.0412553006\n#> 21          s20 0.0375902934\n#> 22          s21 0.0342508754\n#> 23          s22 0.0312081220\n#> 24          s23 0.0284356784\n#> 25          s24 0.0259095311\n#> 26          s25 0.0236077998\n#> 27          s26 0.0215105479\n#> 28          s27 0.0195996101\n#> 29          s28 0.0178584348\n#> 30          s29 0.0162719407\n#> 31          s30 0.0148263863\n#> 32          s31 0.0135092510\n#> 33          s32 0.0123091264\n#> 34          s33 0.0112156175\n#> 35          s34 0.0102192530\n#> 36          s35 0.0093114027\n#> 37          s36 0.0084842034\n#> 38          s37 0.0077304901\n#> 39          s38 0.0070437347\n#> 40          s39 0.0064179887\n#> 41          s40 0.0058478323\n#> 42          s41 0.0053283270\n#> 43          s42 0.0048549731\n#> 44          s43 0.0044236707\n#> 45          s44 0.0040306840\n#> 46          s45 0.0036726091\n#> 47          s46 0.0033463446\n#> 48          s47 0.0030490646\n#> 49          s48 0.0027781941\n#> 50          s49 0.0025313870\n#> 51          s50 0.0023065055\n#> 52          s51 0.0021016020\n#> 53          s52 0.0019149014\n#> 54          s53 0.0017447869\n#> 55          s54 0.0015897848\n#> 56          s55 0.0014485527\n#> 57          s56 0.0013198673\n#> 58          s57 0.0012026139\n#> 59          s58 0.0010957770\n#> 60          s59 0.0009984312\n#> 61          s60 0.0009097333\n#> 62          s61 0.0008289151\n#> 63          s62 0.0007552766\n#> 64          s63 0.0006881799\n#> 65          s64 0.0006270439\n#> 66          s65 0.0005713390\n#> 67          s66 0.0005205829\n#> 68          s67 0.0004743357\n#> 69          s68 0.0004321970\n#> 70          s69 0.0003938018\n#> 71          s70 0.0003588176\n#> 72          s71 0.0003269412\n#> 73          s72 0.0002978966\n#> 74          s73 0.0002714323\n#> 75          s74 0.0002473190\n#> 76          s75 0.0002253479\ndf_for_plot = glmnet_model$beta %>%\n  as.matrix() %>%\n  as.data.frame() %>%\n  rownames_to_column(\"variable\") %>%\n  pivot_longer(cols = -variable, names_to = \"lambda_index\", values_to = \"coef\") %>%\n  left_join(lambda_mapping, by = \"lambda_index\") %>%\n  group_by(variable) %>%\n  arrange(lambda) %>%\n  mutate(lambda_index = 1:n()) %>%\n  ungroup()\ndf_for_plot\n#> # A tibble: 2,280 × 4\n#>    variable lambda_index    coef   lambda\n#>    <chr>           <int>   <dbl>    <dbl>\n#>  1 season              1 -0.0456 0.000225\n#>  2 episode             1  0.133  0.000225\n#>  3 andy                1  0.0319 0.000225\n#>  4 angela              1  0.0734 0.000225\n#>  5 darryl              1  0.0745 0.000225\n#>  6 dwight              1 -0.0168 0.000225\n#>  7 jim                 1  0.108  0.000225\n#>  8 kelly               1 -0.128  0.000225\n#>  9 kevin               1 -0.0471 0.000225\n#> 10 michael             1  0.186  0.000225\n#> # … with 2,270 more rows\ndf_for_plot %>%\n  #filter(variable == \"season\") %>%\n  ggplot(aes(x = lambda, y=coef, col = variable)) +\n  geom_line() +\n  scale_x_log10()+\n  theme_bw()"},{"path":"tree.html","id":"tree","chapter":"3 Tree","heading":"3 Tree","text":"","code":""},{"path":"random-forest.html","id":"random-forest","chapter":"4 Random Forest","heading":"4 Random Forest","text":"","code":""},{"path":"xgboost.html","id":"xgboost","chapter":"5 xgboost","heading":"5 xgboost","text":"","code":""},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"6 Logistic Regression","heading":"6 Logistic Regression","text":"","code":""},{"path":"glmnet-1.html","id":"glmnet-1","chapter":"7 glmnet","heading":"7 glmnet","text":"","code":""},{"path":"tree-1.html","id":"tree-1","chapter":"8 Tree","heading":"8 Tree","text":"","code":""},{"path":"random-forest-1.html","id":"random-forest-1","chapter":"9 Random Forest","heading":"9 Random Forest","text":"","code":""},{"path":"xgboost-1.html","id":"xgboost-1","chapter":"10 xgboost","heading":"10 xgboost","text":"","code":""},{"path":"tree-2.html","id":"tree-2","chapter":"11 Tree","heading":"11 Tree","text":"","code":""}]
