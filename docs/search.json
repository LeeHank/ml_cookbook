[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"自己整理的 ML coding notes","code":""},{"path":"recipes.html","id":"recipes","chapter":"1 Recipes","heading":"1 Recipes","text":"這張圖很好的解釋了recipe這個package的步驟：specify variables:\n這一個步驟，就像寫食譜前，要先標清楚要用哪些食材的意思。\n用recipe()來先標清楚各個variable的type和role。type就是numeric/nominal, role就是outcome/predictor\n最簡單的一種寫法，就是這樣： recipe(y~., data = data)。那透過model formula，系統就知道每個欄位的角色了：y是outcome，其他所有欄位都是predictor。而放進去的data，只是讓系統去知道，每個欄位的type是什麼：factor/character會被判定為nominal, integer/numeric會被判定為numeric。\n對於每個欄位的type和role，都還可以微調，這等等再細講\n這一個步驟，就像寫食譜前，要先標清楚要用哪些食材的意思。用recipe()來先標清楚各個variable的type和role。type就是numeric/nominal, role就是outcome/predictor最簡單的一種寫法，就是這樣： recipe(y~., data = data)。那透過model formula，系統就知道每個欄位的角色了：y是outcome，其他所有欄位都是predictor。而放進去的data，只是讓系統去知道，每個欄位的type是什麼：factor/character會被判定為nominal, integer/numeric會被判定為numeric。對於每個欄位的type和role，都還可以微調，這等等再細講define pre-processing steps:\n這一步驟，就是在寫食譜。把做一道菜的每一個步驟寫好\n用一堆step_* function，來說明to-do事項有哪些。例如我要做one-hot encoding、我要補遺漏值、我要做normalize…\n舉例來說，step_log(total_time, base = 10)，就表示我要對total_time這個欄位取log\n這一步驟，就是在寫食譜。把做一道菜的每一個步驟寫好用一堆step_* function，來說明to-do事項有哪些。例如我要做one-hot encoding、我要補遺漏值、我要做normalize…舉例來說，step_log(total_time, base = 10)，就表示我要對total_time這個欄位取logprovide datasets recipe steps:\n這一步驟，就是做菜前要先備料，的備料階段。\n用prep() function，來prepare所需的材料(例如你要做one-hot encoding時，那個factor的levels到底有哪些？要做normalize時，你的mean和sd要給多少？你要給我一份實際的資料我才知道，這邊通常都是丟training data進去)\n這一步驟，就是做菜前要先備料，的備料階段。用prep() function，來prepare所需的材料(例如你要做one-hot encoding時，那個factor的levels到底有哪些？要做normalize時，你的mean和sd要給多少？你要給我一份實際的資料我才知道，這邊通常都是丟training data進去)apply pre-processing:\n這一步驟，就是實際來做菜了。\n用bake()來實際對給定的資料做preprocessing\n這一步驟，就是實際來做菜了。用bake()來實際對給定的資料做preprocessing","code":"\nlibrary(tidyverse)\nlibrary(recipes)"},{"path":"recipes.html","id":"先快速來個例子","chapter":"1 Recipes","heading":"1.1 先快速來個例子","text":"舉例來說，我有一筆 iris data，我先切成training/testing:那從這筆資料，就可發現有5個變數，其中4個是連續型，一個是類別型那我用 recipe(Species ~., data = iris_train) ，就可定義好這些變數的 type 和 role：可以看到，藉由 data = iris_train，這個recipe物件就可以知道variable有哪 5 個，然後從這5個變數在此data中的型別，就可以得知他的type。接著，formula: Species ~ .，這個recipe物件就可以知道Species的role是outcome，而其他都是predictor特別提醒：\n這邊的formula，不是最後我們要fit model所用的formula。這個formula只是為了定義出各個變數的角色而已。\n這邊的data，也不是我們要fit model所用的data。這邊的data只是讓我知道總共有哪些變數，各個變數的type是什麼而已。所以這邊你要放iris_train, iris_test, iris, 甚至cv的data， 都隨便你，只要這個data裡有你想specify的變數，且這個data各個變數的type都是正確的就好。\n這邊的formula，不是最後我們要fit model所用的formula。這個formula只是為了定義出各個變數的角色而已。這邊的data，也不是我們要fit model所用的data。這邊的data只是讓我知道總共有哪些變數，各個變數的type是什麼而已。所以這邊你要放iris_train, iris_test, iris, 甚至cv的data， 都隨便你，只要這個data裡有你想specify的變數，且這個data各個變數的type都是正確的就好。接下來看第二步驟，我想把type = numeric 的變數，都做normalize:可以看到Operations裡面，寫了Centering scaling all_numeric()接下來，我要prepare我的資料了。我在prep裡面，放入iris_train。那normalize的mean和sd，就都會根據iris_train來計算出來最後，應用到testing set接下來各章，就要來講細節","code":"\nlibrary(rsample)\nset.seed(123)\niris_split = initial_split(iris, strata = Species)\niris_train = training(iris_split)\niris_test = testing(iris_split)\niris_train\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 3            4.7         3.2          1.3         0.2\n#> 4            4.6         3.1          1.5         0.2\n#> 5            5.0         3.6          1.4         0.2\n#> 7            4.6         3.4          1.4         0.3\n#> 8            5.0         3.4          1.5         0.2\n#> 9            4.4         2.9          1.4         0.2\n#> 10           4.9         3.1          1.5         0.1\n#> 11           5.4         3.7          1.5         0.2\n#> 12           4.8         3.4          1.6         0.2\n#> 13           4.8         3.0          1.4         0.1\n#> 14           4.3         3.0          1.1         0.1\n#> 15           5.8         4.0          1.2         0.2\n#> 17           5.4         3.9          1.3         0.4\n#> 18           5.1         3.5          1.4         0.3\n#> 19           5.7         3.8          1.7         0.3\n#> 20           5.1         3.8          1.5         0.3\n#> 21           5.4         3.4          1.7         0.2\n#> 24           5.1         3.3          1.7         0.5\n#> 25           4.8         3.4          1.9         0.2\n#> 26           5.0         3.0          1.6         0.2\n#> 27           5.0         3.4          1.6         0.4\n#> 28           5.2         3.5          1.5         0.2\n#> 29           5.2         3.4          1.4         0.2\n#> 30           4.7         3.2          1.6         0.2\n#> 31           4.8         3.1          1.6         0.2\n#> 32           5.4         3.4          1.5         0.4\n#> 33           5.2         4.1          1.5         0.1\n#> 36           5.0         3.2          1.2         0.2\n#> 37           5.5         3.5          1.3         0.2\n#> 40           5.1         3.4          1.5         0.2\n#> 41           5.0         3.5          1.3         0.3\n#> 42           4.5         2.3          1.3         0.3\n#> 43           4.4         3.2          1.3         0.2\n#> 45           5.1         3.8          1.9         0.4\n#> 48           4.6         3.2          1.4         0.2\n#> 49           5.3         3.7          1.5         0.2\n#> 50           5.0         3.3          1.4         0.2\n#> 52           6.4         3.2          4.5         1.5\n#> 54           5.5         2.3          4.0         1.3\n#> 55           6.5         2.8          4.6         1.5\n#> 56           5.7         2.8          4.5         1.3\n#> 57           6.3         3.3          4.7         1.6\n#> 58           4.9         2.4          3.3         1.0\n#> 59           6.6         2.9          4.6         1.3\n#> 61           5.0         2.0          3.5         1.0\n#> 62           5.9         3.0          4.2         1.5\n#> 63           6.0         2.2          4.0         1.0\n#> 65           5.6         2.9          3.6         1.3\n#> 66           6.7         3.1          4.4         1.4\n#> 67           5.6         3.0          4.5         1.5\n#> 68           5.8         2.7          4.1         1.0\n#> 69           6.2         2.2          4.5         1.5\n#> 70           5.6         2.5          3.9         1.1\n#> 71           5.9         3.2          4.8         1.8\n#> 72           6.1         2.8          4.0         1.3\n#> 73           6.3         2.5          4.9         1.5\n#> 75           6.4         2.9          4.3         1.3\n#> 76           6.6         3.0          4.4         1.4\n#> 77           6.8         2.8          4.8         1.4\n#> 78           6.7         3.0          5.0         1.7\n#> 79           6.0         2.9          4.5         1.5\n#> 82           5.5         2.4          3.7         1.0\n#> 83           5.8         2.7          3.9         1.2\n#> 84           6.0         2.7          5.1         1.6\n#> 87           6.7         3.1          4.7         1.5\n#> 88           6.3         2.3          4.4         1.3\n#> 89           5.6         3.0          4.1         1.3\n#> 91           5.5         2.6          4.4         1.2\n#> 93           5.8         2.6          4.0         1.2\n#> 95           5.6         2.7          4.2         1.3\n#> 96           5.7         3.0          4.2         1.2\n#> 98           6.2         2.9          4.3         1.3\n#> 99           5.1         2.5          3.0         1.1\n#> 100          5.7         2.8          4.1         1.3\n#> 101          6.3         3.3          6.0         2.5\n#> 102          5.8         2.7          5.1         1.9\n#> 103          7.1         3.0          5.9         2.1\n#> 104          6.3         2.9          5.6         1.8\n#> 105          6.5         3.0          5.8         2.2\n#> 107          4.9         2.5          4.5         1.7\n#> 108          7.3         2.9          6.3         1.8\n#> 110          7.2         3.6          6.1         2.5\n#> 112          6.4         2.7          5.3         1.9\n#> 114          5.7         2.5          5.0         2.0\n#> 115          5.8         2.8          5.1         2.4\n#> 118          7.7         3.8          6.7         2.2\n#> 119          7.7         2.6          6.9         2.3\n#> 120          6.0         2.2          5.0         1.5\n#> 121          6.9         3.2          5.7         2.3\n#> 122          5.6         2.8          4.9         2.0\n#> 123          7.7         2.8          6.7         2.0\n#> 125          6.7         3.3          5.7         2.1\n#> 126          7.2         3.2          6.0         1.8\n#> 127          6.2         2.8          4.8         1.8\n#> 129          6.4         2.8          5.6         2.1\n#> 130          7.2         3.0          5.8         1.6\n#> 131          7.4         2.8          6.1         1.9\n#> 132          7.9         3.8          6.4         2.0\n#> 135          6.1         2.6          5.6         1.4\n#> 136          7.7         3.0          6.1         2.3\n#> 139          6.0         3.0          4.8         1.8\n#> 140          6.9         3.1          5.4         2.1\n#> 141          6.7         3.1          5.6         2.4\n#> 142          6.9         3.1          5.1         2.3\n#> 143          5.8         2.7          5.1         1.9\n#> 144          6.8         3.2          5.9         2.3\n#> 146          6.7         3.0          5.2         2.3\n#> 147          6.3         2.5          5.0         1.9\n#> 148          6.5         3.0          5.2         2.0\n#> 149          6.2         3.4          5.4         2.3\n#> 150          5.9         3.0          5.1         1.8\n#>        Species\n#> 3       setosa\n#> 4       setosa\n#> 5       setosa\n#> 7       setosa\n#> 8       setosa\n#> 9       setosa\n#> 10      setosa\n#> 11      setosa\n#> 12      setosa\n#> 13      setosa\n#> 14      setosa\n#> 15      setosa\n#> 17      setosa\n#> 18      setosa\n#> 19      setosa\n#> 20      setosa\n#> 21      setosa\n#> 24      setosa\n#> 25      setosa\n#> 26      setosa\n#> 27      setosa\n#> 28      setosa\n#> 29      setosa\n#> 30      setosa\n#> 31      setosa\n#> 32      setosa\n#> 33      setosa\n#> 36      setosa\n#> 37      setosa\n#> 40      setosa\n#> 41      setosa\n#> 42      setosa\n#> 43      setosa\n#> 45      setosa\n#> 48      setosa\n#> 49      setosa\n#> 50      setosa\n#> 52  versicolor\n#> 54  versicolor\n#> 55  versicolor\n#> 56  versicolor\n#> 57  versicolor\n#> 58  versicolor\n#> 59  versicolor\n#> 61  versicolor\n#> 62  versicolor\n#> 63  versicolor\n#> 65  versicolor\n#> 66  versicolor\n#> 67  versicolor\n#> 68  versicolor\n#> 69  versicolor\n#> 70  versicolor\n#> 71  versicolor\n#> 72  versicolor\n#> 73  versicolor\n#> 75  versicolor\n#> 76  versicolor\n#> 77  versicolor\n#> 78  versicolor\n#> 79  versicolor\n#> 82  versicolor\n#> 83  versicolor\n#> 84  versicolor\n#> 87  versicolor\n#> 88  versicolor\n#> 89  versicolor\n#> 91  versicolor\n#> 93  versicolor\n#> 95  versicolor\n#> 96  versicolor\n#> 98  versicolor\n#> 99  versicolor\n#> 100 versicolor\n#> 101  virginica\n#> 102  virginica\n#> 103  virginica\n#> 104  virginica\n#> 105  virginica\n#> 107  virginica\n#> 108  virginica\n#> 110  virginica\n#> 112  virginica\n#> 114  virginica\n#> 115  virginica\n#> 118  virginica\n#> 119  virginica\n#> 120  virginica\n#> 121  virginica\n#> 122  virginica\n#> 123  virginica\n#> 125  virginica\n#> 126  virginica\n#> 127  virginica\n#> 129  virginica\n#> 130  virginica\n#> 131  virginica\n#> 132  virginica\n#> 135  virginica\n#> 136  virginica\n#> 139  virginica\n#> 140  virginica\n#> 141  virginica\n#> 142  virginica\n#> 143  virginica\n#> 144  virginica\n#> 146  virginica\n#> 147  virginica\n#> 148  virginica\n#> 149  virginica\n#> 150  virginica\niris_recipe <- recipe(Species ~ ., data = iris_train)\nsummary(iris_recipe)\n#> # A tibble: 5 × 4\n#>   variable     type    role      source  \n#>   <chr>        <chr>   <chr>     <chr>   \n#> 1 Sepal.Length numeric predictor original\n#> 2 Sepal.Width  numeric predictor original\n#> 3 Petal.Length numeric predictor original\n#> 4 Petal.Width  numeric predictor original\n#> 5 Species      nominal outcome   original\niris_recipe <- recipe(Species ~ ., data = iris_train) %>%\n  step_normalize(all_numeric())\n\niris_recipe\n#> Recipe\n#> \n#> Inputs:\n#> \n#>       role #variables\n#>    outcome          1\n#>  predictor          4\n#> \n#> Operations:\n#> \n#> Centering and scaling for all_numeric()\niris_rec_prep <- iris_recipe %>% \n  prep(training = iris_train)\niris_rec_prep %>%\n  bake(new_data = iris_test)\n#> # A tibble: 39 × 5\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1       -0.886      1.11          -1.33       -1.30 setosa \n#>  2       -1.12      -0.0744        -1.33       -1.30 setosa \n#>  3       -0.534      2.05          -1.17       -1.04 setosa \n#>  4       -0.182      3.23          -1.28       -1.04 setosa \n#>  5       -0.886      1.58          -1.28       -1.04 setosa \n#>  6       -1.47       1.34          -1.56       -1.30 setosa \n#>  7       -0.417      2.76          -1.33       -1.30 setosa \n#>  8       -1.12       0.162         -1.28       -1.30 setosa \n#>  9       -1.12       1.34          -1.33       -1.43 setosa \n#> 10       -1.71      -0.0744        -1.39       -1.30 setosa \n#> # … with 29 more rows"},{"path":"recipes.html","id":"specify-variables","chapter":"1 Recipes","heading":"1.2 Specify Variables","text":"","code":""},{"path":"recipes.html","id":"定義各個變數的-type-和-role","chapter":"1 Recipes","heading":"1.2.1 定義各個變數的 type 和 role","text":"這一步其實就像是資料庫裡面給schema meta-data 的感覺，我們要去定義每個變數的：\ntype: 這個變數的type是nominal continuous\nrole: 這個變數的角色是 predictor response\ntype: 這個變數的type是nominal continuousrole: 這個變數的角色是 predictor response那最簡單的寫法，就是給他 formula + 一組資料，這樣就能快速的搞定每個variable的type和role。舉例來說，我有一筆 iris data:那從這筆資料，就可發現有5個變數，其中4個是連續型，一個是類別型那我用 recipe(Species ~., data = iris) ，就可定義好這些變數的 type 和 role：可以看到，藉由 data = iris，這個recipe物件就可以知道variable有哪 5 個，然後從這5個變數在此data中的型別，就可以得知他的type。接著，formula: Species ~ .，這個recipe物件就可以知道Species的role是outcome，而其他都是predictor特別提醒：\n這邊的formula，不是最後我們要fit model所用的formula。這個formula只是為了定義出各個變數的角色而已。\n這邊的data，也不是我們要fit model所用的data。這邊的data只是讓我知道總共有哪些變數，各個變數的type是什麼而已。所以這邊你要放training data, testing data, total_data, cv_data 都隨便你，只要這個data裡有你想specify的變數，且這個data各個變數的type都是正確的就好。\n這邊的formula，不是最後我們要fit model所用的formula。這個formula只是為了定義出各個變數的角色而已。這邊的data，也不是我們要fit model所用的data。這邊的data只是讓我知道總共有哪些變數，各個變數的type是什麼而已。所以這邊你要放training data, testing data, total_data, cv_data 都隨便你，只要這個data裡有你想specify的變數，且這個data各個變數的type都是正確的就好。有了這個概念後，就可以來舉一反三了，例如，我想指名Species是predictor就好：我想指名 Sepal.Length & Sepal.Width 是 outcome，其他都是 predictor:我覺得大家都是 predictor我不想指定role，只想指定type","code":"\niris\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 1            5.1         3.5          1.4         0.2\n#> 2            4.9         3.0          1.4         0.2\n#> 3            4.7         3.2          1.3         0.2\n#> 4            4.6         3.1          1.5         0.2\n#> 5            5.0         3.6          1.4         0.2\n#> 6            5.4         3.9          1.7         0.4\n#> 7            4.6         3.4          1.4         0.3\n#> 8            5.0         3.4          1.5         0.2\n#> 9            4.4         2.9          1.4         0.2\n#> 10           4.9         3.1          1.5         0.1\n#> 11           5.4         3.7          1.5         0.2\n#> 12           4.8         3.4          1.6         0.2\n#> 13           4.8         3.0          1.4         0.1\n#> 14           4.3         3.0          1.1         0.1\n#> 15           5.8         4.0          1.2         0.2\n#> 16           5.7         4.4          1.5         0.4\n#> 17           5.4         3.9          1.3         0.4\n#> 18           5.1         3.5          1.4         0.3\n#> 19           5.7         3.8          1.7         0.3\n#> 20           5.1         3.8          1.5         0.3\n#> 21           5.4         3.4          1.7         0.2\n#> 22           5.1         3.7          1.5         0.4\n#> 23           4.6         3.6          1.0         0.2\n#> 24           5.1         3.3          1.7         0.5\n#> 25           4.8         3.4          1.9         0.2\n#> 26           5.0         3.0          1.6         0.2\n#> 27           5.0         3.4          1.6         0.4\n#> 28           5.2         3.5          1.5         0.2\n#> 29           5.2         3.4          1.4         0.2\n#> 30           4.7         3.2          1.6         0.2\n#> 31           4.8         3.1          1.6         0.2\n#> 32           5.4         3.4          1.5         0.4\n#> 33           5.2         4.1          1.5         0.1\n#> 34           5.5         4.2          1.4         0.2\n#> 35           4.9         3.1          1.5         0.2\n#> 36           5.0         3.2          1.2         0.2\n#> 37           5.5         3.5          1.3         0.2\n#> 38           4.9         3.6          1.4         0.1\n#> 39           4.4         3.0          1.3         0.2\n#> 40           5.1         3.4          1.5         0.2\n#> 41           5.0         3.5          1.3         0.3\n#> 42           4.5         2.3          1.3         0.3\n#> 43           4.4         3.2          1.3         0.2\n#> 44           5.0         3.5          1.6         0.6\n#> 45           5.1         3.8          1.9         0.4\n#> 46           4.8         3.0          1.4         0.3\n#> 47           5.1         3.8          1.6         0.2\n#> 48           4.6         3.2          1.4         0.2\n#> 49           5.3         3.7          1.5         0.2\n#> 50           5.0         3.3          1.4         0.2\n#> 51           7.0         3.2          4.7         1.4\n#> 52           6.4         3.2          4.5         1.5\n#> 53           6.9         3.1          4.9         1.5\n#> 54           5.5         2.3          4.0         1.3\n#> 55           6.5         2.8          4.6         1.5\n#> 56           5.7         2.8          4.5         1.3\n#> 57           6.3         3.3          4.7         1.6\n#> 58           4.9         2.4          3.3         1.0\n#> 59           6.6         2.9          4.6         1.3\n#> 60           5.2         2.7          3.9         1.4\n#> 61           5.0         2.0          3.5         1.0\n#> 62           5.9         3.0          4.2         1.5\n#> 63           6.0         2.2          4.0         1.0\n#> 64           6.1         2.9          4.7         1.4\n#> 65           5.6         2.9          3.6         1.3\n#> 66           6.7         3.1          4.4         1.4\n#> 67           5.6         3.0          4.5         1.5\n#> 68           5.8         2.7          4.1         1.0\n#> 69           6.2         2.2          4.5         1.5\n#> 70           5.6         2.5          3.9         1.1\n#> 71           5.9         3.2          4.8         1.8\n#> 72           6.1         2.8          4.0         1.3\n#> 73           6.3         2.5          4.9         1.5\n#> 74           6.1         2.8          4.7         1.2\n#> 75           6.4         2.9          4.3         1.3\n#> 76           6.6         3.0          4.4         1.4\n#> 77           6.8         2.8          4.8         1.4\n#> 78           6.7         3.0          5.0         1.7\n#> 79           6.0         2.9          4.5         1.5\n#> 80           5.7         2.6          3.5         1.0\n#> 81           5.5         2.4          3.8         1.1\n#> 82           5.5         2.4          3.7         1.0\n#> 83           5.8         2.7          3.9         1.2\n#> 84           6.0         2.7          5.1         1.6\n#> 85           5.4         3.0          4.5         1.5\n#> 86           6.0         3.4          4.5         1.6\n#> 87           6.7         3.1          4.7         1.5\n#> 88           6.3         2.3          4.4         1.3\n#> 89           5.6         3.0          4.1         1.3\n#> 90           5.5         2.5          4.0         1.3\n#> 91           5.5         2.6          4.4         1.2\n#> 92           6.1         3.0          4.6         1.4\n#> 93           5.8         2.6          4.0         1.2\n#> 94           5.0         2.3          3.3         1.0\n#> 95           5.6         2.7          4.2         1.3\n#> 96           5.7         3.0          4.2         1.2\n#> 97           5.7         2.9          4.2         1.3\n#> 98           6.2         2.9          4.3         1.3\n#> 99           5.1         2.5          3.0         1.1\n#> 100          5.7         2.8          4.1         1.3\n#> 101          6.3         3.3          6.0         2.5\n#> 102          5.8         2.7          5.1         1.9\n#> 103          7.1         3.0          5.9         2.1\n#> 104          6.3         2.9          5.6         1.8\n#> 105          6.5         3.0          5.8         2.2\n#> 106          7.6         3.0          6.6         2.1\n#> 107          4.9         2.5          4.5         1.7\n#> 108          7.3         2.9          6.3         1.8\n#> 109          6.7         2.5          5.8         1.8\n#> 110          7.2         3.6          6.1         2.5\n#> 111          6.5         3.2          5.1         2.0\n#> 112          6.4         2.7          5.3         1.9\n#> 113          6.8         3.0          5.5         2.1\n#> 114          5.7         2.5          5.0         2.0\n#> 115          5.8         2.8          5.1         2.4\n#> 116          6.4         3.2          5.3         2.3\n#> 117          6.5         3.0          5.5         1.8\n#> 118          7.7         3.8          6.7         2.2\n#> 119          7.7         2.6          6.9         2.3\n#> 120          6.0         2.2          5.0         1.5\n#> 121          6.9         3.2          5.7         2.3\n#> 122          5.6         2.8          4.9         2.0\n#> 123          7.7         2.8          6.7         2.0\n#> 124          6.3         2.7          4.9         1.8\n#> 125          6.7         3.3          5.7         2.1\n#> 126          7.2         3.2          6.0         1.8\n#> 127          6.2         2.8          4.8         1.8\n#> 128          6.1         3.0          4.9         1.8\n#> 129          6.4         2.8          5.6         2.1\n#> 130          7.2         3.0          5.8         1.6\n#> 131          7.4         2.8          6.1         1.9\n#> 132          7.9         3.8          6.4         2.0\n#> 133          6.4         2.8          5.6         2.2\n#> 134          6.3         2.8          5.1         1.5\n#> 135          6.1         2.6          5.6         1.4\n#> 136          7.7         3.0          6.1         2.3\n#> 137          6.3         3.4          5.6         2.4\n#> 138          6.4         3.1          5.5         1.8\n#> 139          6.0         3.0          4.8         1.8\n#> 140          6.9         3.1          5.4         2.1\n#> 141          6.7         3.1          5.6         2.4\n#> 142          6.9         3.1          5.1         2.3\n#> 143          5.8         2.7          5.1         1.9\n#> 144          6.8         3.2          5.9         2.3\n#> 145          6.7         3.3          5.7         2.5\n#> 146          6.7         3.0          5.2         2.3\n#> 147          6.3         2.5          5.0         1.9\n#> 148          6.5         3.0          5.2         2.0\n#> 149          6.2         3.4          5.4         2.3\n#> 150          5.9         3.0          5.1         1.8\n#>        Species\n#> 1       setosa\n#> 2       setosa\n#> 3       setosa\n#> 4       setosa\n#> 5       setosa\n#> 6       setosa\n#> 7       setosa\n#> 8       setosa\n#> 9       setosa\n#> 10      setosa\n#> 11      setosa\n#> 12      setosa\n#> 13      setosa\n#> 14      setosa\n#> 15      setosa\n#> 16      setosa\n#> 17      setosa\n#> 18      setosa\n#> 19      setosa\n#> 20      setosa\n#> 21      setosa\n#> 22      setosa\n#> 23      setosa\n#> 24      setosa\n#> 25      setosa\n#> 26      setosa\n#> 27      setosa\n#> 28      setosa\n#> 29      setosa\n#> 30      setosa\n#> 31      setosa\n#> 32      setosa\n#> 33      setosa\n#> 34      setosa\n#> 35      setosa\n#> 36      setosa\n#> 37      setosa\n#> 38      setosa\n#> 39      setosa\n#> 40      setosa\n#> 41      setosa\n#> 42      setosa\n#> 43      setosa\n#> 44      setosa\n#> 45      setosa\n#> 46      setosa\n#> 47      setosa\n#> 48      setosa\n#> 49      setosa\n#> 50      setosa\n#> 51  versicolor\n#> 52  versicolor\n#> 53  versicolor\n#> 54  versicolor\n#> 55  versicolor\n#> 56  versicolor\n#> 57  versicolor\n#> 58  versicolor\n#> 59  versicolor\n#> 60  versicolor\n#> 61  versicolor\n#> 62  versicolor\n#> 63  versicolor\n#> 64  versicolor\n#> 65  versicolor\n#> 66  versicolor\n#> 67  versicolor\n#> 68  versicolor\n#> 69  versicolor\n#> 70  versicolor\n#> 71  versicolor\n#> 72  versicolor\n#> 73  versicolor\n#> 74  versicolor\n#> 75  versicolor\n#> 76  versicolor\n#> 77  versicolor\n#> 78  versicolor\n#> 79  versicolor\n#> 80  versicolor\n#> 81  versicolor\n#> 82  versicolor\n#> 83  versicolor\n#> 84  versicolor\n#> 85  versicolor\n#> 86  versicolor\n#> 87  versicolor\n#> 88  versicolor\n#> 89  versicolor\n#> 90  versicolor\n#> 91  versicolor\n#> 92  versicolor\n#> 93  versicolor\n#> 94  versicolor\n#> 95  versicolor\n#> 96  versicolor\n#> 97  versicolor\n#> 98  versicolor\n#> 99  versicolor\n#> 100 versicolor\n#> 101  virginica\n#> 102  virginica\n#> 103  virginica\n#> 104  virginica\n#> 105  virginica\n#> 106  virginica\n#> 107  virginica\n#> 108  virginica\n#> 109  virginica\n#> 110  virginica\n#> 111  virginica\n#> 112  virginica\n#> 113  virginica\n#> 114  virginica\n#> 115  virginica\n#> 116  virginica\n#> 117  virginica\n#> 118  virginica\n#> 119  virginica\n#> 120  virginica\n#> 121  virginica\n#> 122  virginica\n#> 123  virginica\n#> 124  virginica\n#> 125  virginica\n#> 126  virginica\n#> 127  virginica\n#> 128  virginica\n#> 129  virginica\n#> 130  virginica\n#> 131  virginica\n#> 132  virginica\n#> 133  virginica\n#> 134  virginica\n#> 135  virginica\n#> 136  virginica\n#> 137  virginica\n#> 138  virginica\n#> 139  virginica\n#> 140  virginica\n#> 141  virginica\n#> 142  virginica\n#> 143  virginica\n#> 144  virginica\n#> 145  virginica\n#> 146  virginica\n#> 147  virginica\n#> 148  virginica\n#> 149  virginica\n#> 150  virginica\niris_recipe <- recipe(Species ~ ., data = iris)\nsummary(iris_recipe)\n#> # A tibble: 5 × 4\n#>   variable     type    role      source  \n#>   <chr>        <chr>   <chr>     <chr>   \n#> 1 Sepal.Length numeric predictor original\n#> 2 Sepal.Width  numeric predictor original\n#> 3 Petal.Length numeric predictor original\n#> 4 Petal.Width  numeric predictor original\n#> 5 Species      nominal outcome   original\nrecipe(~Species, data = iris) %>% summary()\n#> # A tibble: 1 × 4\n#>   variable type    role      source  \n#>   <chr>    <chr>   <chr>     <chr>   \n#> 1 Species  nominal predictor original\nrecipe(Sepal.Length + Sepal.Width ~ ., data = iris) %>% summary()\n#> # A tibble: 5 × 4\n#>   variable     type    role      source  \n#>   <chr>        <chr>   <chr>     <chr>   \n#> 1 Petal.Length numeric predictor original\n#> 2 Petal.Width  numeric predictor original\n#> 3 Species      nominal predictor original\n#> 4 Sepal.Length numeric outcome   original\n#> 5 Sepal.Width  numeric outcome   original\nrecipe(~ ., data = iris) %>% summary()\n#> # A tibble: 5 × 4\n#>   variable     type    role      source  \n#>   <chr>        <chr>   <chr>     <chr>   \n#> 1 Sepal.Length numeric predictor original\n#> 2 Sepal.Width  numeric predictor original\n#> 3 Petal.Length numeric predictor original\n#> 4 Petal.Width  numeric predictor original\n#> 5 Species      nominal predictor original\nrecipe(iris) %>% summary()\n#> # A tibble: 5 × 4\n#>   variable     type    role  source  \n#>   <chr>        <chr>   <lgl> <chr>   \n#> 1 Sepal.Length numeric NA    original\n#> 2 Sepal.Width  numeric NA    original\n#> 3 Petal.Length numeric NA    original\n#> 4 Petal.Width  numeric NA    original\n#> 5 Species      nominal NA    original"},{"path":"recipes.html","id":"update_role","chapter":"1 Recipes","heading":"1.2.2 update_role","text":"如果我想更新某些變數的角色，那我可以用 update_role()舉例來說，剛剛的iris data，我目前的recipe長這樣：那我如果突然不想做supervised learning了，我想做做分群或PCA等unsupervised learning，那我就把outcome的role，轉成predictor甚至，這些變數其實也不是predictor了，因為在unsupervised learning裡，根本沒predictor這種東西。那我可以更新他們的角色，叫做”characteristic”。這個名稱隨便你取，你取啥都可以：","code":"\niris_recipe <- recipe(Species ~ ., data = iris)\nsummary(iris_recipe)\n#> # A tibble: 5 × 4\n#>   variable     type    role      source  \n#>   <chr>        <chr>   <chr>     <chr>   \n#> 1 Sepal.Length numeric predictor original\n#> 2 Sepal.Width  numeric predictor original\n#> 3 Petal.Length numeric predictor original\n#> 4 Petal.Width  numeric predictor original\n#> 5 Species      nominal outcome   original\niris_recipe %>%\n  update_role(Species, new_role = \"predictor\") %>%\n  summary()\n#> # A tibble: 5 × 4\n#>   variable     type    role      source  \n#>   <chr>        <chr>   <chr>     <chr>   \n#> 1 Sepal.Length numeric predictor original\n#> 2 Sepal.Width  numeric predictor original\n#> 3 Petal.Length numeric predictor original\n#> 4 Petal.Width  numeric predictor original\n#> 5 Species      nominal predictor original\niris_recipe %>%\n  update_role(everything(), new_role = \"characteristic\") %>%\n  summary()\n#> # A tibble: 5 × 4\n#>   variable     type    role           source  \n#>   <chr>        <chr>   <chr>          <chr>   \n#> 1 Sepal.Length numeric characteristic original\n#> 2 Sepal.Width  numeric characteristic original\n#> 3 Petal.Length numeric characteristic original\n#> 4 Petal.Width  numeric characteristic original\n#> 5 Species      nominal characteristic original"},{"path":"recipes.html","id":"role-inheritance","chapter":"1 Recipes","heading":"1.2.3 role inheritance","text":"等等會學很多step_function，他會改變我原先的變數。那變數改變後，他的角色會變怎樣？答案是，角色會繼承(預設)。或是，你在做step_function時，直接assign給他新角色。可以看到，Species已經被轉成Species_versicolor和Species_virginica了。會轉成這兩個level，是靠prep()裡面的data來轉的。而轉完的role，仍然是predictor。而source可以看到，從original變成derived，因為這是後續生出來的變數那另一個例子是，我做step_dummy時，直接assign給他新角色，例如：這邊就可以看到，本來Species在最一開始，是predictor的角色，但我在做step_dummy時，assign他轉換後的結果要是”trousers”這個角色。那轉完就可以看到role的確是 trousers","code":"\nrecipe( ~ ., data = iris) %>% \n  step_dummy(Species) %>% \n  prep(training = iris) %>% \n  summary()\n#> # A tibble: 6 × 4\n#>   variable           type    role      source  \n#>   <chr>              <chr>   <chr>     <chr>   \n#> 1 Sepal.Length       numeric predictor original\n#> 2 Sepal.Width        numeric predictor original\n#> 3 Petal.Length       numeric predictor original\n#> 4 Petal.Width        numeric predictor original\n#> 5 Species_versicolor numeric predictor derived \n#> 6 Species_virginica  numeric predictor derived\nrecipe( ~ ., data = iris) %>% \n  step_dummy(Species, role = \"trousers\") %>% \n  prep() %>% \n  summary()\n#> # A tibble: 6 × 4\n#>   variable           type    role      source  \n#>   <chr>              <chr>   <chr>     <chr>   \n#> 1 Sepal.Length       numeric predictor original\n#> 2 Sepal.Width        numeric predictor original\n#> 3 Petal.Length       numeric predictor original\n#> 4 Petal.Width        numeric predictor original\n#> 5 Species_versicolor numeric trousers  derived \n#> 6 Species_virginica  numeric trousers  derived"},{"path":"recipes.html","id":"selecting-variables","chapter":"1 Recipes","heading":"1.3 Selecting Variables","text":"上一章定義完變數的meta-data後，現在每個變數都具有三個特徵：\n這個變數的名字\n這個變數的type\n這個變數的role\n這個變數的名字這個變數的type這個變數的role所以，我在選擇變數的時候，我就可以善用這三個特徵來選變數","code":""},{"path":"recipes.html","id":"用名字選變數","chapter":"1 Recipes","heading":"1.3.1 用名字選變數","text":"舉例來說，我的step_dummy，想做在 species 上，那我可以寫：我也可以用dplyr的starts_with(), end_with(),…來選變數。例如：","code":"\nrecipe(Species ~ ., data = iris) %>%\n  step_dummy(Species) %>%\n  prep() %>%\n  juice()\n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width\n#>           <dbl>       <dbl>        <dbl>       <dbl>\n#>  1          5.1         3.5          1.4         0.2\n#>  2          4.9         3            1.4         0.2\n#>  3          4.7         3.2          1.3         0.2\n#>  4          4.6         3.1          1.5         0.2\n#>  5          5           3.6          1.4         0.2\n#>  6          5.4         3.9          1.7         0.4\n#>  7          4.6         3.4          1.4         0.3\n#>  8          5           3.4          1.5         0.2\n#>  9          4.4         2.9          1.4         0.2\n#> 10          4.9         3.1          1.5         0.1\n#> # … with 140 more rows, and 2 more variables:\n#> #   Species_versicolor <dbl>, Species_virginica <dbl>\nrecipe(Species ~ ., data = iris) %>%\n  step_normalize(starts_with(\"Sepal\")) %>%\n  prep() %>%\n  juice()\n#> # A tibble: 150 × 5\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1       -0.898      1.02            1.4         0.2 setosa \n#>  2       -1.14      -0.132           1.4         0.2 setosa \n#>  3       -1.38       0.327           1.3         0.2 setosa \n#>  4       -1.50       0.0979          1.5         0.2 setosa \n#>  5       -1.02       1.25            1.4         0.2 setosa \n#>  6       -0.535      1.93            1.7         0.4 setosa \n#>  7       -1.50       0.786           1.4         0.3 setosa \n#>  8       -1.02       0.786           1.5         0.2 setosa \n#>  9       -1.74      -0.361           1.4         0.2 setosa \n#> 10       -1.14       0.0979          1.5         0.1 setosa \n#> # … with 140 more rows"},{"path":"recipes.html","id":"用type選變數","chapter":"1 Recipes","heading":"1.3.2 用type選變數","text":"變數的type有nominal和 numeric，所以，我可以這樣做：","code":""},{"path":"recipes.html","id":"all_numeric","chapter":"1 Recipes","heading":"1.3.2.1 all_numeric()","text":"","code":"\nrecipe(Species ~ ., data = iris) %>%\n  step_normalize(all_numeric()) %>%\n  prep() %>%\n  juice()\n#> # A tibble: 150 × 5\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1       -0.898      1.02          -1.34       -1.31 setosa \n#>  2       -1.14      -0.132         -1.34       -1.31 setosa \n#>  3       -1.38       0.327         -1.39       -1.31 setosa \n#>  4       -1.50       0.0979        -1.28       -1.31 setosa \n#>  5       -1.02       1.25          -1.34       -1.31 setosa \n#>  6       -0.535      1.93          -1.17       -1.05 setosa \n#>  7       -1.50       0.786         -1.34       -1.18 setosa \n#>  8       -1.02       0.786         -1.28       -1.31 setosa \n#>  9       -1.74      -0.361         -1.34       -1.31 setosa \n#> 10       -1.14       0.0979        -1.28       -1.44 setosa \n#> # … with 140 more rows"},{"path":"recipes.html","id":"all_nominal","chapter":"1 Recipes","heading":"1.3.2.2 all_nominal()","text":"","code":"\nrecipe(Species ~ ., data = iris) %>%\n  step_dummy(all_nominal()) %>%\n  prep() %>%\n  juice()\n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width\n#>           <dbl>       <dbl>        <dbl>       <dbl>\n#>  1          5.1         3.5          1.4         0.2\n#>  2          4.9         3            1.4         0.2\n#>  3          4.7         3.2          1.3         0.2\n#>  4          4.6         3.1          1.5         0.2\n#>  5          5           3.6          1.4         0.2\n#>  6          5.4         3.9          1.7         0.4\n#>  7          4.6         3.4          1.4         0.3\n#>  8          5           3.4          1.5         0.2\n#>  9          4.4         2.9          1.4         0.2\n#> 10          4.9         3.1          1.5         0.1\n#> # … with 140 more rows, and 2 more variables:\n#> #   Species_versicolor <dbl>, Species_virginica <dbl>"},{"path":"recipes.html","id":"用role選變數","chapter":"1 Recipes","heading":"1.3.3 用role選變數","text":"","code":""},{"path":"recipes.html","id":"all_predictors","chapter":"1 Recipes","heading":"1.3.3.1 all_predictors()","text":"","code":"\nrecipe(Species ~ ., data = iris) %>%\n  step_normalize(all_predictors()) %>%\n  prep() %>%\n  juice()\n#> # A tibble: 150 × 5\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1       -0.898      1.02          -1.34       -1.31 setosa \n#>  2       -1.14      -0.132         -1.34       -1.31 setosa \n#>  3       -1.38       0.327         -1.39       -1.31 setosa \n#>  4       -1.50       0.0979        -1.28       -1.31 setosa \n#>  5       -1.02       1.25          -1.34       -1.31 setosa \n#>  6       -0.535      1.93          -1.17       -1.05 setosa \n#>  7       -1.50       0.786         -1.34       -1.18 setosa \n#>  8       -1.02       0.786         -1.28       -1.31 setosa \n#>  9       -1.74      -0.361         -1.34       -1.31 setosa \n#> 10       -1.14       0.0979        -1.28       -1.44 setosa \n#> # … with 140 more rows"},{"path":"recipes.html","id":"all_outcomes","chapter":"1 Recipes","heading":"1.3.3.2 all_outcomes()","text":"","code":"\nrecipe(Species ~ ., data = iris) %>%\n  step_dummy(all_outcomes()) %>%\n  prep() %>%\n  juice()\n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width\n#>           <dbl>       <dbl>        <dbl>       <dbl>\n#>  1          5.1         3.5          1.4         0.2\n#>  2          4.9         3            1.4         0.2\n#>  3          4.7         3.2          1.3         0.2\n#>  4          4.6         3.1          1.5         0.2\n#>  5          5           3.6          1.4         0.2\n#>  6          5.4         3.9          1.7         0.4\n#>  7          4.6         3.4          1.4         0.3\n#>  8          5           3.4          1.5         0.2\n#>  9          4.4         2.9          1.4         0.2\n#> 10          4.9         3.1          1.5         0.1\n#> # … with 140 more rows, and 2 more variables:\n#> #   Species_versicolor <dbl>, Species_virginica <dbl>"},{"path":"recipes.html","id":"has_role","chapter":"1 Recipes","heading":"1.3.3.3 has_role()","text":"","code":"\nrecipe(Species ~ ., \n       data = iris %>% mutate(money = rnorm(n = 150, mean = 100000, sd = 1000))) %>%\n  update_role(money, new_role = \"whatever\") %>%\n  step_log(has_role(\"whatever\")) %>%\n  prep() %>%\n  juice()\n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width money\n#>           <dbl>       <dbl>        <dbl>       <dbl> <dbl>\n#>  1          5.1         3.5          1.4         0.2  11.5\n#>  2          4.9         3            1.4         0.2  11.5\n#>  3          4.7         3.2          1.3         0.2  11.5\n#>  4          4.6         3.1          1.5         0.2  11.5\n#>  5          5           3.6          1.4         0.2  11.5\n#>  6          5.4         3.9          1.7         0.4  11.5\n#>  7          4.6         3.4          1.4         0.3  11.5\n#>  8          5           3.4          1.5         0.2  11.5\n#>  9          4.4         2.9          1.4         0.2  11.5\n#> 10          4.9         3.1          1.5         0.1  11.5\n#> # … with 140 more rows, and 1 more variable: Species <fct>"},{"path":"recipes.html","id":"全部參雜在一起選變數","chapter":"1 Recipes","heading":"1.3.4 全部參雜在一起選變數","text":"例如我可以選全部的 “type = numeric” “role != outcome”的變數，做log處理也有一些 compund selectors，例如： all_nominal_predictors() all_numeric_predictors()","code":"\nrecipe(money ~ ., data = iris %>% mutate(money = rnorm(150, 100000, 1000))) %>%\n  step_normalize(all_numeric(), -all_outcomes()) %>%\n  prep() %>%\n  juice()\n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1       -0.898      1.02          -1.34       -1.31 setosa \n#>  2       -1.14      -0.132         -1.34       -1.31 setosa \n#>  3       -1.38       0.327         -1.39       -1.31 setosa \n#>  4       -1.50       0.0979        -1.28       -1.31 setosa \n#>  5       -1.02       1.25          -1.34       -1.31 setosa \n#>  6       -0.535      1.93          -1.17       -1.05 setosa \n#>  7       -1.50       0.786         -1.34       -1.18 setosa \n#>  8       -1.02       0.786         -1.28       -1.31 setosa \n#>  9       -1.74      -0.361         -1.34       -1.31 setosa \n#> 10       -1.14       0.0979        -1.28       -1.44 setosa \n#> # … with 140 more rows, and 1 more variable: money <dbl>"},{"path":"recipes.html","id":"define-preprocessing-steps","chapter":"1 Recipes","heading":"1.4 Define Preprocessing Steps","text":"","code":""},{"path":"recipes.html","id":"impute-missing-values","chapter":"1 Recipes","heading":"1.4.1 Impute missing values","text":"","code":""},{"path":"recipes.html","id":"step_unknown","chapter":"1 Recipes","heading":"1.4.1.1 step_unknown","text":"如果 cat1 是個類別變數，然後資料裡面有 missing，我們可以用 step_unknown()，把missing的地方幫他補成 “unknown”也可以加上 new_level = \"xxx\"，來把 unknown 換成你喜歡的 level","code":"\ndf_train = data.frame(\n  cat1 = c(\"A\",\"A\",\"B\",NA, \"C\"),\n  cont1 = 1:5\n)\nmy_unknown = recipe(~., data = df_train) %>%\n  step_unknown(cat1) %>%\n  prep(training = df_train)\n\nbake(my_unknown, df_train)\n#> # A tibble: 5 × 2\n#>   cat1    cont1\n#>   <fct>   <int>\n#> 1 A           1\n#> 2 A           2\n#> 3 B           3\n#> 4 unknown     4\n#> 5 C           5\nmy_unknown2 = recipe(~., data = df_train) %>%\n  step_unknown(cat1, new_level = \"missing_catcat\") %>%\n  prep(training = df_train)\n\nbake(my_unknown2, df_train)\n#> # A tibble: 5 × 2\n#>   cat1           cont1\n#>   <fct>          <int>\n#> 1 A                  1\n#> 2 A                  2\n#> 3 B                  3\n#> 4 missing_catcat     4\n#> 5 C                  5"},{"path":"recipes.html","id":"對-categorical-variables常見的處理","chapter":"1 Recipes","heading":"1.4.2 對 categorical variables常見的處理","text":"","code":""},{"path":"recipes.html","id":"step_dummy","chapter":"1 Recipes","heading":"1.4.2.1 step_dummy","text":"","code":""},{"path":"recipes.html","id":"dummy-coding","chapter":"1 Recipes","heading":"1.4.2.1.1 dummy coding","text":"雖然，很多package都有內建這個功能(e.g. lm()會自動把factor變數做dummy)，但各個package常常有不一致的地方：\n有的package用dummy coding，有的用one-hot encoding\n各個package在做dummy/one-hot時，命名方式也不一樣\n有的package用dummy coding，有的用one-hot encoding各個package在做dummy/one-hot時，命名方式也不一樣所以，我們可以在前處理的時候，用recipe先把類別變數都統一的轉好，那丟到不同model的engine時，就不用再管之後彼此會出現不一致了繼續舉剛剛的例子，並且先建立一個original欄位，：做轉換：","code":"\niris2 <- iris %>% mutate(Species_old = Species)\niris_dummy = recipe( ~ ., data = iris2) %>% \n  step_dummy(Species) %>% \n  prep(training = iris2)\nsummary(iris_dummy)\n#> # A tibble: 7 × 4\n#>   variable           type    role      source  \n#>   <chr>              <chr>   <chr>     <chr>   \n#> 1 Sepal.Length       numeric predictor original\n#> 2 Sepal.Width        numeric predictor original\n#> 3 Petal.Length       numeric predictor original\n#> 4 Petal.Width        numeric predictor original\n#> 5 Species_old        nominal predictor original\n#> 6 Species_versicolor numeric predictor derived \n#> 7 Species_virginica  numeric predictor derived\niris_dummy %>%\n  bake(new_data = iris2) %>%\n  select(Species_old, starts_with(\"Species\")) %>%\n  distinct()\n#> # A tibble: 3 × 3\n#>   Species_old Species_versicolor Species_virginica\n#>   <fct>                    <dbl>             <dbl>\n#> 1 setosa                       0                 0\n#> 2 versicolor                   1                 0\n#> 3 virginica                    0                 1"},{"path":"recipes.html","id":"contrast","chapter":"1 Recipes","heading":"1.4.2.1.2 contrast","text":"在實驗設計裡，還有不同的dummy coding方式，例如sum coding, helmert coding, …和 dummy 的差別是，他coding完可能是用 sum = 0 的方式 (e.g. coding成 c(-1, 0, 1))那這部分在 recipe 的網站上有範例 (ARTICLES/DUMMY VARIABLES …)，我這邊很少用就先不整理了","code":""},{"path":"recipes.html","id":"one-hot-coding","chapter":"1 Recipes","heading":"1.4.2.1.3 one-hot coding","text":"加上 one_hot = TRUE 就好","code":"\niris2 <- iris %>% mutate(Species_old = Species)\niris_onehot = recipe( ~ ., data = iris2) %>% \n  step_dummy(Species, one_hot = TRUE) %>% \n  prep(training = iris2)\niris_onehot %>%\n  bake(new_data = iris2) %>%\n  select(Species_old, starts_with(\"Species\")) %>%\n  distinct()\n#> # A tibble: 3 × 4\n#>   Species_old Species_setosa Species_versicolor\n#>   <fct>                <dbl>              <dbl>\n#> 1 setosa                   1                  0\n#> 2 versicolor               0                  1\n#> 3 virginica                0                  0\n#> # … with 1 more variable: Species_virginica <dbl>"},{"path":"recipes.html","id":"step_other","chapter":"1 Recipes","heading":"1.4.2.2 step_other","text":"對於類別變數，我們可以把出現頻率較少的level，全都整併成 others如果bake的時候，餵進來的變數，有新的level，那他也會被併到 others 裡面以下舉例：上面這個資料集，training data的cat1有4個level，而C和D是很少出現的類別(各只有5%)我們如果用step_dummy()，就可以把小於threshold的類別合併為others，例如：那如果我今天的test data裡面，cat1這個變數的分佈長這樣：那我套用剛剛的 recipe 會怎樣？ 答案是 C, D 會被轉成others(因為剛剛的prep()後，已經確定讓C, D都是others)，而且，E沒看過，也會變 others：最後，幾點提醒：\n如果本來的類別變數，裡面的level就有名字叫 ，那這樣做會跳error\n解決辦法是， step_other()裡面可以下這個參數 = \"你自己訂other要叫啥\"，那就可以訂出你喜歡的other名稱\n如果本來的類別變數，裡面的level就有名字叫 ，那這樣做會跳error解決辦法是， step_other()裡面可以下這個參數 = \"你自己訂other要叫啥\"，那就可以訂出你喜歡的other名稱","code":"\ndf_train = data.frame(\n  cat1 = c(rep(\"A\",50), rep(\"B\",40), rep(\"C\", 5), rep(\"D\", 5)),\n  cont1 = rnorm(100)\n)\n\ndf_train %>%\n  count(cat1) %>%\n  mutate(p = prop.table(n))\n#>   cat1  n    p\n#> 1    A 50 0.50\n#> 2    B 40 0.40\n#> 3    C  5 0.05\n#> 4    D  5 0.05\nmy_other = recipe(~., data = df_train) %>%\n  step_other(cat1, threshold = 0.1) %>%\n  prep(train = df_train)\n\nmy_other %>%\n  bake(new_data = df_train) %>%\n  count(cat1)\n#> # A tibble: 3 × 2\n#>   cat1      n\n#>   <fct> <int>\n#> 1 A        50\n#> 2 B        40\n#> 3 other    10\ndf_test = data.frame(\n  cat1 = c(rep(\"A\",2), rep(\"B\",2), rep(\"C\", 2), rep(\"D\", 2), rep(\"E\",2)),\n  cont1 = rnorm(10)\n)\ndf_test %>%\n  count(cat1) %>%\n  mutate(p = prop.table(n))\n#>   cat1 n   p\n#> 1    A 2 0.2\n#> 2    B 2 0.2\n#> 3    C 2 0.2\n#> 4    D 2 0.2\n#> 5    E 2 0.2\nmy_other %>%\n  bake(new_data = df_test) %>%\n  count(cat1)\n#> # A tibble: 3 × 2\n#>   cat1      n\n#>   <fct> <int>\n#> 1 A         2\n#> 2 B         2\n#> 3 other     6"},{"path":"recipes.html","id":"step_bin2factor","chapter":"1 Recipes","heading":"1.4.2.3 step_bin2factor","text":"","code":""},{"path":"recipes.html","id":"對-continuous-variables常見的處理","chapter":"1 Recipes","heading":"1.4.3 對 continuous variables常見的處理","text":"","code":""},{"path":"recipes.html","id":"step_center","chapter":"1 Recipes","heading":"1.4.3.1 step_center","text":"","code":""},{"path":"recipes.html","id":"step_scale","chapter":"1 Recipes","heading":"1.4.3.2 step_scale","text":"","code":""},{"path":"recipes.html","id":"step_normalize","chapter":"1 Recipes","heading":"1.4.3.3 step_normalize","text":"","code":""},{"path":"recipes.html","id":"step_corr共線處理","chapter":"1 Recipes","heading":"1.4.3.4 step_corr(共線處理)","text":"第一個常見的前處理，是砍掉mulit-colinearity的欄位。最簡單的做法，就是numeric variable間兩兩做correlation，然後，把correlation > 某個threshold(e.g. 0.9)的，擇一留下就好第一個常見的前處理，是砍掉mulit-colinearity的欄位。最簡單的做法，就是numeric variable間兩兩做correlation，然後，把correlation > 某個threshold(e.g. 0.9)的，擇一留下就好舉例來說，iris data 的 correlation matrix為：舉例來說，iris data 的 correlation matrix為：可以看到Petal.Length 和 Sepal.Length 的相關高達 0.87那在recipe中，可以這樣寫，把相關大於0.85的拔掉：可以看到，他會把Petal.Length拔掉","code":"\niris %>%\n  select(-Species) %>%\n  cor()\n#>              Sepal.Length Sepal.Width Petal.Length\n#> Sepal.Length    1.0000000  -0.1175698    0.8717538\n#> Sepal.Width    -0.1175698   1.0000000   -0.4284401\n#> Petal.Length    0.8717538  -0.4284401    1.0000000\n#> Petal.Width     0.8179411  -0.3661259    0.9628654\n#>              Petal.Width\n#> Sepal.Length   0.8179411\n#> Sepal.Width   -0.3661259\n#> Petal.Length   0.9628654\n#> Petal.Width    1.0000000\niris_cor = recipe(~ ., data = iris) %>%\n  step_corr(all_numeric_predictors(), threshold = 0.85) %>%\n  prep(training = iris)\n\nsummary(iris_cor)\n#> # A tibble: 4 × 4\n#>   variable     type    role      source  \n#>   <chr>        <chr>   <chr>     <chr>   \n#> 1 Sepal.Length numeric predictor original\n#> 2 Sepal.Width  numeric predictor original\n#> 3 Petal.Width  numeric predictor original\n#> 4 Species      nominal predictor original"},{"path":"recipes.html","id":"交互作用","chapter":"1 Recipes","heading":"1.4.4 交互作用","text":"","code":""},{"path":"recipes.html","id":"step_interact","chapter":"1 Recipes","heading":"1.4.4.1 step_interact","text":"直接講結論，等等再講原理：\n交互作用都要是連續型變數之間才能做，所以要先把類別變數都step_dummy後，才做交互作用\n以 iris data 為例，如果要做類別變數 Species 和 Sepal.Length 的交互作用，那就寫成：\n交互作用都要是連續型變數之間才能做，所以要先把類別變數都step_dummy後，才做交互作用以 iris data 為例，如果要做類別變數 Species 和 Sepal.Length 的交互作用，那就寫成：這邊的技巧在於，用 starts_with(\"Species\")，就可以把Species轉成dummy後的變數全抓出來，然後一一和Sepal.Length做交互作用那如果你很暴力，想把類別變數全轉成dummy，然後所有解釋變數都做交互作用，你可以這樣做：來講點原理： 之前直接用linear model時，交互作用其實是用 model.matrix() 在製作的，例如這樣寫：那就會發現，之前做linear model時，也是先幫你轉dummy，才去相乘的。所以就更放心recipe這樣處理的合理性","code":"\niris_int <- \n  recipe( ~ ., data = iris) %>% \n  step_dummy(Species) %>% # 先轉 dummy\n  step_interact( ~ starts_with(\"Species\"):Sepal.Length) %>%\n  prep(training = iris)\nsummary(iris_int)\n#> # A tibble: 8 × 4\n#>   variable                          type    role      source\n#>   <chr>                             <chr>   <chr>     <chr> \n#> 1 Sepal.Length                      numeric predictor origi…\n#> 2 Sepal.Width                       numeric predictor origi…\n#> 3 Petal.Length                      numeric predictor origi…\n#> 4 Petal.Width                       numeric predictor origi…\n#> 5 Species_versicolor                numeric predictor deriv…\n#> 6 Species_virginica                 numeric predictor deriv…\n#> 7 Species_versicolor_x_Sepal.Length numeric predictor deriv…\n#> 8 Species_virginica_x_Sepal.Length  numeric predictor deriv…\niris_int_all <- \n  recipe( ~ ., data = iris) %>% \n  step_dummy(all_nominal()) %>% # 先轉 dummy\n  step_interact(~all_predictors():all_predictors()) %>%\n  prep(training = iris)\nsummary(iris_int_all)\n#> # A tibble: 21 × 4\n#>    variable                          type    role    source \n#>    <chr>                             <chr>   <chr>   <chr>  \n#>  1 Sepal.Length                      numeric predic… origin…\n#>  2 Sepal.Width                       numeric predic… origin…\n#>  3 Petal.Length                      numeric predic… origin…\n#>  4 Petal.Width                       numeric predic… origin…\n#>  5 Species_versicolor                numeric predic… derived\n#>  6 Species_virginica                 numeric predic… derived\n#>  7 Sepal.Length_x_Sepal.Width        numeric predic… derived\n#>  8 Sepal.Length_x_Petal.Length       numeric predic… derived\n#>  9 Sepal.Length_x_Petal.Width        numeric predic… derived\n#> 10 Sepal.Length_x_Species_versicolor numeric predic… derived\n#> # … with 11 more rows\nmodel.matrix(~ Species*Sepal.Length, data = iris) %>% \n  as.data.frame() %>% \n  # show a few specific rows\n  slice(c(1, 51, 101)) %>% \n  as.data.frame()\n#>     (Intercept) Speciesversicolor Speciesvirginica\n#> 1             1                 0                0\n#> 51            1                 1                0\n#> 101           1                 0                1\n#>     Sepal.Length Speciesversicolor:Sepal.Length\n#> 1            5.1                              0\n#> 51           7.0                              7\n#> 101          6.3                              0\n#>     Speciesvirginica:Sepal.Length\n#> 1                             0.0\n#> 51                            0.0\n#> 101                           6.3"},{"path":"recipes.html","id":"移除不必要欄位","chapter":"1 Recipes","heading":"1.4.5 移除不必要欄位","text":"","code":""},{"path":"recipes.html","id":"step_zv","chapter":"1 Recipes","heading":"1.4.5.1 step_zv","text":"","code":""},{"path":"recipes.html","id":"step_rm","chapter":"1 Recipes","heading":"1.4.5.2 step_rm","text":"","code":""},{"path":"recipes.html","id":"transformation","chapter":"1 Recipes","heading":"1.4.6 transformation","text":"","code":""},{"path":"recipes.html","id":"step_mutate","chapter":"1 Recipes","heading":"1.4.6.1 step_mutate","text":"就是借用dplyr的mutate，如下：可以看到新增的變數，role會繼承原本的role。","code":"\nrec <-\n  recipe( ~ ., data = iris) %>%\n  step_mutate(\n    dbl_width = Sepal.Width * 2,\n    half_length = Sepal.Length / 2\n  )\n\nprepped <- prep(rec, training = iris %>% slice(1:75))\nsummary(prepped)\n#> # A tibble: 7 × 4\n#>   variable     type    role      source  \n#>   <chr>        <chr>   <chr>     <chr>   \n#> 1 Sepal.Length numeric predictor original\n#> 2 Sepal.Width  numeric predictor original\n#> 3 Petal.Length numeric predictor original\n#> 4 Petal.Width  numeric predictor original\n#> 5 Species      nominal predictor original\n#> 6 dbl_width    numeric predictor derived \n#> 7 half_length  numeric predictor derived"},{"path":"recipes.html","id":"step_log","chapter":"1 Recipes","heading":"1.4.6.2 step_log","text":"這邊懶得整理了，幾點提醒一下：\nstep_log()的預設底數是exp(1)，所以是ln轉換。如果要改，可以這樣改step_log(xx, base = 10)\n如果step_log()的變數<= 0，會幫你轉成NA\n那要避免log(0)的問題，可以加offset，寫成 step_log(xx, offset = 1)，那就是log(xx+1)的意思\nstep_log()的預設底數是exp(1)，所以是ln轉換。如果要改，可以這樣改step_log(xx, base = 10)如果step_log()的變數<= 0，會幫你轉成NA那要避免log(0)的問題，可以加offset，寫成 step_log(xx, offset = 1)，那就是log(xx+1)的意思","code":""},{"path":"recipes.html","id":"step_logit","chapter":"1 Recipes","heading":"1.4.6.3 step_logit","text":"就是做這種轉換： f(p) = log(p/(1-p)","code":"\nset.seed(313)\nexamples <- data.frame(\n  matrix(runif(40), ncol = 2)\n)\n\nlogit_trans <- recipe(~ X1 + X2, data = examples) %>%\n  step_logit(all_predictors()) %>%\n  prep(training = examples)\n\ntransformed_te <- bake(logit_trans, examples)\nplot(examples$X1, transformed_te$X1)"},{"path":"recipes.html","id":"step_boxcox","chapter":"1 Recipes","heading":"1.4.6.4 step_BoxCox","text":"","code":""},{"path":"recipes.html","id":"step_yeojohnson","chapter":"1 Recipes","heading":"1.4.6.5 step_YeoJohnson","text":"","code":""},{"path":"recipes.html","id":"step_ns","chapter":"1 Recipes","heading":"1.4.6.6 step_ns","text":"","code":""},{"path":"recipes.html","id":"step_poly","chapter":"1 Recipes","heading":"1.4.6.7 step_poly","text":"","code":""},{"path":"recipes.html","id":"日期時間類","chapter":"1 Recipes","heading":"1.4.7 日期時間類","text":"","code":""},{"path":"recipes.html","id":"順序很重要","chapter":"1 Recipes","heading":"1.4.8 順序很重要","text":"剛剛介紹的一堆 step_xxx() ，他是沿著 pipeline 一路處理下來的，所以順序很重要例如，你如果先做 step_normalize() ， 再做 step_log()，那就準備GG，因為你會餵負值進去又例如，原本的類別變數裡有missing，你應該先做 step_unknown()，之後再做step_dummy()。但如果你倒過來的話，那先做 step_dummy就會先送你NA了，後續你做step_unknwon()也做不到這個類別變數上(因為現在沒有這個類別變數了，剩下dummy後的連續變數)所以，以下是 recipe 網頁建議的通用step順序，可以參考看看：\nImpute\nHandle factor levels\nIndividual transformations skewness issues\nDiscretize (needed choice)\nCreate dummy variables\nCreate interactions\nNormalization steps (center, scale, range, etc)\nMultivariate transformation (e.g. PCA, spatial sign, etc)\nImputeHandle factor levelsIndividual transformations skewness issuesDiscretize (needed choice)Create dummy variablesCreate interactionsNormalization steps (center, scale, range, etc)Multivariate transformation (e.g. PCA, spatial sign, etc)那對於5,6,7,8的順序是有爭議的，因為上面這種建議作法，完全是以預測為考量，不管解釋的。但另一派認為，step_dummy()，再做 step_normalize()，那原本的類別變數轉成dummy的0,1你還可以解釋，但一normalize完，你根本無法解釋這個數值是啥意思了。所以也有人認為應該先 7, 8，再做 5, 6 (我也比較傾向這樣做)","code":""},{"path":"recipes.html","id":"經典-model-的前處理套路","chapter":"1 Recipes","heading":"1.4.9 經典 model 的前處理套路","text":"","code":""},{"path":"recipes.html","id":"linear-model","chapter":"1 Recipes","heading":"1.4.9.1 Linear model","text":"","code":""},{"path":"recipes.html","id":"lasso","chapter":"1 Recipes","heading":"1.4.9.2 Lasso","text":"","code":""},{"path":"recipes.html","id":"random-forest","chapter":"1 Recipes","heading":"1.4.9.3 random forest","text":"","code":""},{"path":"recipes.html","id":"xgboost","chapter":"1 Recipes","heading":"1.4.9.4 xgboost","text":"","code":""},{"path":"recipes.html","id":"checks","chapter":"1 Recipes","heading":"1.4.10 CHECKS","text":"recipe網站的首頁 SIMPLE EXAMPLE 最下面，有介紹一下 check 類的function，可以幫你 check missing, class, cols, name, …等東西之後有空再整理","code":""},{"path":"recipes.html","id":"客製化自己的-step_function","chapter":"1 Recipes","heading":"1.4.11 客製化自己的 step_function","text":"","code":""},{"path":"linear-model-1.html","id":"linear-model-1","chapter":"2 Linear Model","heading":"2 Linear Model","text":"","code":""},{"path":"glmnet.html","id":"glmnet","chapter":"3 glmnet","heading":"3 glmnet","text":"","code":""},{"path":"glmnet.html","id":"理論","chapter":"3 glmnet","heading":"3.1 理論","text":"","code":""},{"path":"glmnet.html","id":"lasso-實作","chapter":"3 glmnet","heading":"3.2 lasso 實作","text":"這篇文章摘自 Julia Silge 在 2020/5/17 寫的 blog 連結主要目的，是用 lasso regression，去預測 Office 這個美國暢銷影集的某集 IMDB ratings","code":"\nlibrary(tidyverse)"},{"path":"glmnet.html","id":"explore-the-data","chapter":"3 glmnet","heading":"3.2.1 Explore the data","text":"要拿來預測用的資料，n = 135, p = 32變數說明：\nimdb_rating: 要預測的目標 y (連續型)\nepisode_name 是 ID 欄位\nseason 有被我當 predictor，因為我猜不同季的 rating會不同(例如一開始rating還好，到中間口碑變很好所以rating高，到最後幾季又開始拖戲和爛尾所以 rating 低)\nepisode 也有被我當 predictor, 因為我猜不同 episode 的 rating 可能也不同 (例如每季剛開播，跟接近結束，可能 rating 較高，中間 rating 較低)\nandy, angela, …, justin_spitzer 共 28 個變數，都是人名，裡面的數值，表示該演員在這一集裡面，講過多少次話。這些也被我當 predictor，我猜有些演員很討喜，他講越多話 rating 可能越好。\nimdb_rating: 要預測的目標 y (連續型)episode_name 是 ID 欄位season 有被我當 predictor，因為我猜不同季的 rating會不同(例如一開始rating還好，到中間口碑變很好所以rating高，到最後幾季又開始拖戲和爛尾所以 rating 低)episode 也有被我當 predictor, 因為我猜不同 episode 的 rating 可能也不同 (例如每季剛開播，跟接近結束，可能 rating 較高，中間 rating 較低)andy, angela, …, justin_spitzer 共 28 個變數，都是人名，裡面的數值，表示該演員在這一集裡面，講過多少次話。這些也被我當 predictor，我猜有些演員很討喜，他講越多話 rating 可能越好。整體看一下，有沒有 missing，以及分佈的狀況：Table 3.1: Data summaryVariable type: characterVariable type: numeric全部欄位都沒有 missing， y 的分佈蠻不錯的常態這邊再做一個 EDA ，看看 episode 和 rating有沒有關係 (是不是每一季越到後面的集數，rating會越高？)看起來的確有這個趨勢啊！","code":"\n# ratings_raw <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv\")\n# \n# remove_regex = \"[:punct:]|[:digit:]|parts |part |the |and\"\n# \n# office_ratings <- ratings_raw %>%\n#   transmute(\n#     episode_name = str_to_lower(title),\n#     episode_name = str_remove_all(\n#       episode_name, remove_regex\n#     ),\n#     episode_name = str_trim(episode_name),\n#     imdb_rating\n#   )\n# \n# office_info <- schrute::theoffice %>%\n#   mutate(\n#     season = as.numeric(season),\n#     episode = as.numeric(episode),\n#     episode_name = str_to_lower(episode_name),\n#     episode_name = str_remove_all(episode_name, remove_regex),\n#     episode_name = str_trim(episode_name)\n#   ) %>%\n#   select(season, episode, episode_name, director, writer, character)\n# \n# characters <- office_info %>%\n#   count(episode_name, character) %>%\n#   add_count(character, wt = n, name = \"character_count\") %>%\n#   filter(character_count > 800) %>%\n#   select(-character_count) %>%\n#   pivot_wider(\n#     names_from = character,\n#     values_from = n,\n#     values_fill = list(n = 0)\n#   )\n# \n# creators <- office_info %>%\n#   distinct(episode_name, director, writer) %>%\n#   pivot_longer(director:writer, names_to = \"role\", values_to = \"person\") %>%\n#   separate_rows(person, sep = \";\") %>%\n#   add_count(person) %>%\n#   filter(n > 10) %>%\n#   distinct(episode_name, person) %>%\n#   mutate(person_value = 1) %>%\n#   pivot_wider(\n#     names_from = person,\n#     values_from = person_value,\n#     values_fill = list(person_value = 0)\n#   )\n# \n# office <- office_info %>%\n#   distinct(season, episode, episode_name) %>%\n#   inner_join(characters) %>%\n#   inner_join(creators) %>%\n#   inner_join(office_ratings %>%\n#     select(episode_name, imdb_rating)) %>%\n#   janitor::clean_names()\n# \n# saveRDS(office, \"model_example/glmnet/office.rds\")\noffice = readRDS(\"./data/office.rds\")\noffice\n#> # A tibble: 136 × 32\n#>    season episode episode_name     andy angela darryl dwight\n#>     <dbl>   <dbl> <chr>           <int>  <int>  <int>  <int>\n#>  1      1       1 pilot               0      1      0     29\n#>  2      1       2 diversity day       0      4      0     17\n#>  3      1       3 health care         0      5      0     62\n#>  4      1       5 basketball          0      3     15     25\n#>  5      1       6 hot girl            0      3      0     28\n#>  6      2       1 dundies             0      1      1     32\n#>  7      2       2 sexual harassm…     0      2      9     11\n#>  8      2       3 office olympics     0      6      0     55\n#>  9      2       4 fire                0     17      0     65\n#> 10      2       5 halloween           0     13      0     33\n#> # … with 126 more rows, and 25 more variables: jim <int>,\n#> #   kelly <int>, kevin <int>, michael <int>, oscar <int>,\n#> #   pam <int>, phyllis <int>, ryan <int>, toby <int>,\n#> #   erin <int>, jan <int>, ken_kwapis <dbl>,\n#> #   greg_daniels <dbl>, b_j_novak <dbl>,\n#> #   paul_lieberstein <dbl>, mindy_kaling <dbl>,\n#> #   paul_feig <dbl>, gene_stupnitsky <dbl>, …\nskimr::skim(office)\noffice %>%\n  ggplot(aes(episode, imdb_rating, fill = as.factor(episode))) +\n  geom_boxplot(show.legend = FALSE)"},{"path":"glmnet.html","id":"train-a-model","chapter":"3 glmnet","heading":"3.2.2 Train a model","text":"","code":""},{"path":"glmnet.html","id":"split-data","chapter":"3 glmnet","heading":"3.2.2.1 split data","text":"","code":"\nlibrary(rsample)\nset.seed(1234)\noffice_split <- initial_split(office, strata = season)\noffice_train <- training(office_split)\noffice_test <- testing(office_split)\n\n# 資料太少，不用 cv ，改用 bootstrap\nset.seed(1234)\noffice_boot <- bootstraps(office_train, strata = season)"},{"path":"glmnet.html","id":"preprocessing","chapter":"3 glmnet","heading":"3.2.2.2 preprocessing","text":"","code":"\nlibrary(recipes)\noffice_rec <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(episode_name, new_role = \"ID\") %>% # episode 不把他當 predictor\n  step_zv(all_numeric(), -all_outcomes()) %>% # zero variance 對 回歸問題都會造成影響\n  step_normalize(all_numeric(), -all_outcomes()) # lasso 需要做 normalize"},{"path":"glmnet.html","id":"specify-model","chapter":"3 glmnet","heading":"3.2.2.3 specify model","text":"可以看到，訂 model 的時候，我 specify 他的 penalty 要用 tuning 的，然後 mixture設為1 (就會是 lasso)","code":"\nlibrary(parsnip)\nlibrary(tune)\nlasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")"},{"path":"glmnet.html","id":"workflow-setting","chapter":"3 glmnet","heading":"3.2.2.4 workflow setting","text":"","code":"\nlibrary(workflows)\nlasso_wf = workflow() %>%\n  add_recipe(office_rec) %>%\n  add_model(lasso_spec)\n\nlasso_wf\n#> ══ Workflow ════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: linear_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────\n#> 2 Recipe Steps\n#> \n#> • step_zv()\n#> • step_normalize()\n#> \n#> ── Model ───────────────────────────────────────────────────\n#> Linear Regression Model Specification (regression)\n#> \n#> Main Arguments:\n#>   penalty = tune()\n#>   mixture = 1\n#> \n#> Computational engine: glmnet"},{"path":"glmnet.html","id":"hyper-parameter-setting","chapter":"3 glmnet","heading":"3.2.2.5 hyper parameter setting","text":"從 hyper parameter 的 meta data table，可看出：\nidentifier: penalty ，是這個超參數的 id\nobject: nparam[+] 的意思是，他是 numeric parameter，+ 表示已有設定 range 在裡面\nidentifier: penalty ，是這個超參數的 idobject: nparam[+] 的意思是，他是 numeric parameter，+ 表示已有設定 range 在裡面我們可以這樣看到他的 range，是在 log10 的尺度下，從 -10 到 0我想用 latin_hypercube，幫他在 feature space 中均勻撒 50 個點","code":"\nlibrary(dials)\nhyper_param_meta = lasso_spec %>%\n  parameters() %>%\n  finalize(office_train)\n\nhyper_param_meta\n#> Collection of 1 parameters for tuning\n#> \n#>  identifier    type    object\n#>     penalty penalty nparam[+]\nhyper_param_meta %>%\n  pull_dials_object(\"penalty\")\n#> Amount of Regularization (quantitative)\n#> Transformer:  log-10 \n#> Range (transformed scale): [-10, 0]\nmy_grid = grid_latin_hypercube(hyper_param_meta, size = 50)\nmy_grid %>%\n  arrange(penalty)\n#> # A tibble: 50 × 1\n#>     penalty\n#>       <dbl>\n#>  1 1.12e-10\n#>  2 2.20e-10\n#>  3 3.64e-10\n#>  4 4.34e-10\n#>  5 7.01e-10\n#>  6 1.42e- 9\n#>  7 1.98e- 9\n#>  8 3.41e- 9\n#>  9 4.02e- 9\n#> 10 9.39e- 9\n#> # … with 40 more rows"},{"path":"glmnet.html","id":"model-fitting","chapter":"3 glmnet","heading":"3.2.2.6 model fitting","text":"","code":""},{"path":"glmnet.html","id":"tune-hyper-parameter","chapter":"3 glmnet","heading":"3.2.2.6.1 tune hyper parameter","text":"看一下 tune 完後，最佳的 penalty 訂為多少是 0.0481234來看一下 tunning 的過程：nice，可以看到 lasso 的確有幫助到結果 (但看 r-square 可以發現頗慘烈，才 15% 而已)","code":"\n# 開平行運算\nlibrary(doParallel)\ncl <- makePSOCKcluster(4) # Create a cluster object\nregisterDoParallel(cl) # register\n\nlibrary(yardstick)\n# fitting\nset.seed(130)\nlasso_tune <- \n  tune::tune_grid(\n    object = lasso_wf,\n    resamples = office_boot,\n    metrics = metric_set(rmse, rsq),\n    grid = my_grid,\n    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n)\n\n# 關平行運算\nstopCluster(cl)\nfinal_param = lasso_tune %>% select_best(metric = \"rmse\", maximize = FALSE)\nfinal_param\n#> # A tibble: 1 × 2\n#>   penalty .config              \n#>     <dbl> <chr>                \n#> 1  0.0481 Preprocessor1_Model44\nlasso_tune %>%\n  collect_metrics() %>%\n  ggplot(aes(x = penalty, y = mean, color = .metric)) +\n  geom_errorbar(\n    aes(ymin = mean - std_err, ymax = mean + std_err), \n    alpha = 0.5\n  ) +\n  geom_line(size = 1.5) +\n  geom_vline(xintercept = final_param$penalty, color = \"blue\", lty = 2) +\n  facet_wrap(~.metric, scales = \"free\", nrow = 2) +\n  scale_x_log10() +\n  theme_bw() +\n  theme(legend.position = \"none\")"},{"path":"glmnet.html","id":"finalize-workflow-model","chapter":"3 glmnet","heading":"3.2.2.7 finalize workflow & model","text":"最後，用這組最佳參數，去 finalize 我們的 model","code":"\nfinal_lasso_wf = lasso_wf %>% finalize_workflow(final_param) # finalized workflow\nfinal_lasso_fit <- final_lasso_wf %>% fit(office_train) # finalized model"},{"path":"glmnet.html","id":"prediction","chapter":"3 glmnet","heading":"3.2.3 Prediction","text":"對測試集做預測","code":"\n# 對測試集做預測\noffice_test_res <- bind_cols(\n  stats::predict(final_lasso_fit, office_test), # 預測值\n  office_test %>% select(imdb_rating) # 真值\n)\noffice_test_res\n#> # A tibble: 36 × 2\n#>    .pred imdb_rating\n#>    <dbl>       <dbl>\n#>  1  8.56         7.6\n#>  2  8.80         8.2\n#>  3  8.33         8.2\n#>  4  8.67         8.2\n#>  5  8.39         7.9\n#>  6  8.46         8.2\n#>  7  8.41         8.3\n#>  8  8.26         8  \n#>  9  8.00         8.2\n#> 10  8.31         8.5\n#> # … with 26 more rows"},{"path":"glmnet.html","id":"evaluation","chapter":"3 glmnet","heading":"3.2.4 Evaluation","text":"hmm… r-square 實在是差強人意啊～","code":"\noffice_metrics <- yardstick::metric_set(rmse, rsq, mae)\noffice_metrics(\n  data = office_test_res, \n  truth = imdb_rating, \n  estimate = .pred\n)\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard       0.428\n#> 2 rsq     standard       0.109\n#> 3 mae     standard       0.343"},{"path":"glmnet.html","id":"explaination","chapter":"3 glmnet","heading":"3.2.5 Explaination","text":"","code":""},{"path":"glmnet.html","id":"變數重要性","chapter":"3 glmnet","heading":"3.2.5.1 變數重要性","text":"先來看一下我們 fit 出的 model:可以看到，最重要的是michael，再來是greg_daniels, 依此類推通常在處理變數重要性時，都會用 vip 這個 package，因為他還有很多額外功能。可以看到，結果和剛剛的係數一模一樣。那這邊特別提醒，vi() 裡面要下 lambda = 這個 argument。我一開始沒有下這個 argument，他就會自動去取最小的lambda值帶入，那就幾乎所有變數的係數都 >0，根本不是我要的。解釋可參考這裡可以看到，結果和剛剛的係數一模一樣。那這邊特別提醒，vi() 裡面要下 lambda = 這個 argument。我一開始沒有下這個 argument，他就會自動去取最小的lambda值帶入，那就幾乎所有變數的係數都 >0，根本不是我要的。解釋可參考這裡那我們幫這個變數重要性畫圖：那我們幫這個變數重要性畫圖：可以看到，最重要的是michael(收視保證啊～)那如果要挑出變數給別人看，就 filter 出係數大於 0 的變數就好：","code":"\n# parsnip model\nparsnip_model = final_lasso_fit %>%\n  fit(office_train) %>%\n  pull_workflow_fit()\ncoef_df = parsnip_model %>%\n  tidy() %>%\n  arrange(desc(abs(estimate)))\n\ncoef_df\n#> # A tibble: 31 × 3\n#>    term            estimate penalty\n#>    <chr>              <dbl>   <dbl>\n#>  1 (Intercept)       8.37    0.0481\n#>  2 michael           0.162   0.0481\n#>  3 greg_daniels      0.101   0.0481\n#>  4 episode           0.0613  0.0481\n#>  5 kelly            -0.0420  0.0481\n#>  6 jim               0.0410  0.0481\n#>  7 jan               0.0406  0.0481\n#>  8 angela            0.0387  0.0481\n#>  9 paul_feig         0.0332  0.0481\n#> 10 randall_einhorn  -0.0277  0.0481\n#> # … with 21 more rows\nlibrary(vip)\nparsnip_model %>%\n  vip::vi(lambda = final_param$penalty)\n#> # A tibble: 30 × 3\n#>    Variable         Importance Sign \n#>    <chr>                 <dbl> <chr>\n#>  1 michael              0.162  POS  \n#>  2 greg_daniels         0.101  POS  \n#>  3 episode              0.0613 POS  \n#>  4 kelly                0.0420 NEG  \n#>  5 jim                  0.0410 POS  \n#>  6 jan                  0.0406 POS  \n#>  7 angela               0.0387 POS  \n#>  8 paul_feig            0.0332 POS  \n#>  9 randall_einhorn      0.0277 NEG  \n#> 10 paul_lieberstein     0.0119 POS  \n#> # … with 20 more rows\nparsnip_model %>%\n  vip::vi(lambda = final_param$penalty) %>%\n  mutate(\n    Importance = abs(Importance),\n    Variable = fct_reorder(Variable, Importance)\n  ) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = NULL)\nparsnip_model %>%\n  tidy() %>%\n  filter(estimate!=0) %>%\n  arrange(desc(abs(estimate)))\n#> # A tibble: 13 × 3\n#>    term             estimate penalty\n#>    <chr>               <dbl>   <dbl>\n#>  1 (Intercept)       8.37     0.0481\n#>  2 michael           0.162    0.0481\n#>  3 greg_daniels      0.101    0.0481\n#>  4 episode           0.0613   0.0481\n#>  5 kelly            -0.0420   0.0481\n#>  6 jim               0.0410   0.0481\n#>  7 jan               0.0406   0.0481\n#>  8 angela            0.0387   0.0481\n#>  9 paul_feig         0.0332   0.0481\n#> 10 randall_einhorn  -0.0277   0.0481\n#> 11 paul_lieberstein  0.0119   0.0481\n#> 12 darryl            0.00653  0.0481\n#> 13 dwight            0.00592  0.0481"},{"path":"glmnet.html","id":"統計推論","chapter":"3 glmnet","heading":"3.2.6 統計推論","text":"如果要做統計推論(哪些變數顯著)，","code":""},{"path":"glmnet.html","id":"模型細節","chapter":"3 glmnet","heading":"3.2.6.1 模型細節","text":"如果你想看 glmnet 的模型細節，可以先取出原生 package 的物件：","code":"\n# 原生 package 的物件\nglmnet_model = parsnip_model$fit"},{"path":"glmnet.html","id":"法一用-broom-幫忙-summarise-推薦","chapter":"3 glmnet","heading":"3.2.6.1.1 法一：用 broom 幫忙 summarise (推薦)","text":"可以看到，每一個 term (變數)，在每一個 step (.e. 對應的 lambda 下)，所得到的係數估計值，以及解釋變異量(dev.ratio, fraction null deviance explained value lambda)他的 step 都是 1:72，從最大的 lambda 到最小的 lambda來畫一下 shrinkage 過程","code":"\nglmnet_model %>%\n  broom::tidy(return_zeros = FALSE)\n#> # A tibble: 1,550 × 5\n#>    term         step estimate lambda dev.ratio\n#>    <chr>       <dbl>    <dbl>  <dbl>     <dbl>\n#>  1 (Intercept)     1     8.37  0.242    0     \n#>  2 (Intercept)     2     8.37  0.220    0.0357\n#>  3 (Intercept)     3     8.37  0.201    0.0654\n#>  4 (Intercept)     4     8.37  0.183    0.0900\n#>  5 (Intercept)     5     8.37  0.167    0.110 \n#>  6 (Intercept)     6     8.37  0.152    0.136 \n#>  7 (Intercept)     7     8.37  0.138    0.162 \n#>  8 (Intercept)     8     8.37  0.126    0.183 \n#>  9 (Intercept)     9     8.37  0.115    0.201 \n#> 10 (Intercept)    10     8.37  0.105    0.223 \n#> # … with 1,540 more rows\ntidied <- tidy(glmnet_model) %>% \n  filter(term != \"(Intercept)\")\n\n tidied %>%\n   ggplot(aes(lambda, estimate, col = term)) +\n   geom_line() +\n   scale_x_log10()"},{"path":"glmnet.html","id":"法二原生-pacage-的做法","chapter":"3 glmnet","heading":"3.2.6.1.2 法二：原生 pacage 的做法","text":"這是 glmnet 的標準output：\n第三欄的 Lambda，就是 penalty 為多少\n第一欄的 Df = degree freedom = 參數個數 = 有幾個變數的係數不等於 0\n第二欄的 %Dev 是指 percent deviance explained\n第一列，是 Lambda 最大時 (0.21)，沒有任何一個變數係數大於0。然後 lambda 慢慢放寬後，越來越多變數的係數大於 0\n第三欄的 Lambda，就是 penalty 為多少第一欄的 Df = degree freedom = 參數個數 = 有幾個變數的係數不等於 0第二欄的 %Dev 是指 percent deviance explained第一列，是 Lambda 最大時 (0.21)，沒有任何一個變數係數大於0。然後 lambda 慢慢放寬後，越來越多變數的係數大於 0如果想看各個變數 shrinkage 的過程，快速的方法可以這樣做：如果想畫美美的圖，就去拿原始資料：lambda mapping 自己做：然後，自己轉成畫圖用資料：","code":"\nglmnet_model\n#> \n#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n#> \n#>    Df  %Dev   Lambda\n#> 1   0  0.00 0.241600\n#> 2   1  3.57 0.220200\n#> 3   1  6.54 0.200600\n#> 4   1  9.00 0.182800\n#> 5   1 11.04 0.166500\n#> 6   2 13.60 0.151800\n#> 7   2 16.16 0.138300\n#> 8   2 18.28 0.126000\n#> 9   3 20.09 0.114800\n#> 10  4 22.33 0.104600\n#> 11  6 24.72 0.095310\n#> 12  8 27.41 0.086840\n#> 13 10 30.23 0.079120\n#> 14 10 33.18 0.072090\n#> 15 10 35.63 0.065690\n#> 16 11 37.69 0.059850\n#> 17 11 39.51 0.054540\n#> 18 12 41.20 0.049690\n#> 19 12 42.66 0.045280\n#> 20 12 43.86 0.041260\n#> 21 12 44.86 0.037590\n#> 22 12 45.69 0.034250\n#> 23 12 46.38 0.031210\n#> 24 12 46.96 0.028440\n#> 25 13 47.52 0.025910\n#> 26 16 48.03 0.023610\n#> 27 18 48.54 0.021510\n#> 28 20 49.08 0.019600\n#> 29 20 49.63 0.017860\n#> 30 20 50.08 0.016270\n#> 31 21 50.65 0.014830\n#> 32 21 51.18 0.013510\n#> 33 21 51.62 0.012310\n#> 34 20 51.98 0.011220\n#> 35 22 52.36 0.010220\n#> 36 22 52.71 0.009311\n#> 37 22 53.00 0.008484\n#> 38 22 53.24 0.007730\n#> 39 22 53.43 0.007044\n#> 40 22 53.60 0.006418\n#> 41 23 53.74 0.005848\n#> 42 24 53.88 0.005328\n#> 43 25 54.02 0.004855\n#> 44 25 54.14 0.004424\n#> 45 25 54.23 0.004031\n#> 46 25 54.31 0.003673\n#> 47 27 54.38 0.003346\n#> 48 27 54.45 0.003049\n#> 49 28 54.52 0.002778\n#> 50 28 54.58 0.002531\n#> 51 28 54.62 0.002307\n#> 52 28 54.66 0.002102\n#> 53 28 54.70 0.001915\n#> 54 28 54.73 0.001745\n#> 55 28 54.75 0.001590\n#> 56 28 54.77 0.001449\n#> 57 28 54.78 0.001320\n#> 58 28 54.80 0.001203\n#> 59 28 54.81 0.001096\n#> 60 28 54.82 0.000998\n#> 61 28 54.82 0.000910\n#> 62 28 54.83 0.000829\n#> 63 28 54.84 0.000755\n#> 64 28 54.84 0.000688\n#> 65 28 54.84 0.000627\n#> 66 28 54.85 0.000571\n#> 67 28 54.85 0.000521\n#> 68 28 54.85 0.000474\n#> 69 28 54.85 0.000432\n#> 70 28 54.86 0.000394\n#> 71 28 54.86 0.000359\n#> 72 28 54.86 0.000327\n#> 73 28 54.86 0.000298\n#> 74 28 54.86 0.000271\n#> 75 28 54.86 0.000247\n#> 76 28 54.86 0.000225\nplot(glmnet_model, xvar = \"lambda\", label = TRUE)\nglmnet_model$beta %>%\n  as.matrix() %>%\n  as.data.frame() %>%\n  .[c(1:5),c(1:5)]\n#>         s0 s1 s2 s3 s4\n#> season   0  0  0  0  0\n#> episode  0  0  0  0  0\n#> andy     0  0  0  0  0\n#> angela   0  0  0  0  0\n#> darryl   0  0  0  0  0\nlambda_mapping = data.frame(\n  lambda_index = paste0(\"s\", 0:(length(glmnet_model$lambda)-1)),\n  lambda = glmnet_model$lambda\n)\nlambda_mapping\n#>    lambda_index       lambda\n#> 1            s0 0.2416331546\n#> 2            s1 0.2201671311\n#> 3            s2 0.2006080900\n#> 4            s3 0.1827866202\n#> 5            s4 0.1665483606\n#> 6            s5 0.1517526633\n#> 7            s6 0.1382713749\n#> 8            s7 0.1259877271\n#> 9            s8 0.1147953247\n#> 10           s9 0.1045972244\n#> 11          s10 0.0953050953\n#> 12          s11 0.0868384533\n#> 13          s12 0.0791239644\n#> 14          s13 0.0720948095\n#> 15          s14 0.0656901053\n#> 16          s15 0.0598543774\n#> 17          s16 0.0545370795\n#> 18          s17 0.0496921557\n#> 19          s18 0.0452776415\n#> 20          s19 0.0412553006\n#> 21          s20 0.0375902934\n#> 22          s21 0.0342508754\n#> 23          s22 0.0312081220\n#> 24          s23 0.0284356784\n#> 25          s24 0.0259095311\n#> 26          s25 0.0236077998\n#> 27          s26 0.0215105479\n#> 28          s27 0.0195996101\n#> 29          s28 0.0178584348\n#> 30          s29 0.0162719407\n#> 31          s30 0.0148263863\n#> 32          s31 0.0135092510\n#> 33          s32 0.0123091264\n#> 34          s33 0.0112156175\n#> 35          s34 0.0102192530\n#> 36          s35 0.0093114027\n#> 37          s36 0.0084842034\n#> 38          s37 0.0077304901\n#> 39          s38 0.0070437347\n#> 40          s39 0.0064179887\n#> 41          s40 0.0058478323\n#> 42          s41 0.0053283270\n#> 43          s42 0.0048549731\n#> 44          s43 0.0044236707\n#> 45          s44 0.0040306840\n#> 46          s45 0.0036726091\n#> 47          s46 0.0033463446\n#> 48          s47 0.0030490646\n#> 49          s48 0.0027781941\n#> 50          s49 0.0025313870\n#> 51          s50 0.0023065055\n#> 52          s51 0.0021016020\n#> 53          s52 0.0019149014\n#> 54          s53 0.0017447869\n#> 55          s54 0.0015897848\n#> 56          s55 0.0014485527\n#> 57          s56 0.0013198673\n#> 58          s57 0.0012026139\n#> 59          s58 0.0010957770\n#> 60          s59 0.0009984312\n#> 61          s60 0.0009097333\n#> 62          s61 0.0008289151\n#> 63          s62 0.0007552766\n#> 64          s63 0.0006881799\n#> 65          s64 0.0006270439\n#> 66          s65 0.0005713390\n#> 67          s66 0.0005205829\n#> 68          s67 0.0004743357\n#> 69          s68 0.0004321970\n#> 70          s69 0.0003938018\n#> 71          s70 0.0003588176\n#> 72          s71 0.0003269412\n#> 73          s72 0.0002978966\n#> 74          s73 0.0002714323\n#> 75          s74 0.0002473190\n#> 76          s75 0.0002253479\ndf_for_plot = glmnet_model$beta %>%\n  as.matrix() %>%\n  as.data.frame() %>%\n  rownames_to_column(\"variable\") %>%\n  pivot_longer(cols = -variable, names_to = \"lambda_index\", values_to = \"coef\") %>%\n  left_join(lambda_mapping, by = \"lambda_index\") %>%\n  group_by(variable) %>%\n  arrange(lambda) %>%\n  mutate(lambda_index = 1:n()) %>%\n  ungroup()\ndf_for_plot\n#> # A tibble: 2,280 × 4\n#>    variable lambda_index    coef   lambda\n#>    <chr>           <int>   <dbl>    <dbl>\n#>  1 season              1 -0.0456 0.000225\n#>  2 episode             1  0.133  0.000225\n#>  3 andy                1  0.0319 0.000225\n#>  4 angela              1  0.0734 0.000225\n#>  5 darryl              1  0.0745 0.000225\n#>  6 dwight              1 -0.0168 0.000225\n#>  7 jim                 1  0.108  0.000225\n#>  8 kelly               1 -0.128  0.000225\n#>  9 kevin               1 -0.0471 0.000225\n#> 10 michael             1  0.186  0.000225\n#> # … with 2,270 more rows\ndf_for_plot %>%\n  #filter(variable == \"season\") %>%\n  ggplot(aes(x = lambda, y=coef, col = variable)) +\n  geom_line() +\n  scale_x_log10()+\n  theme_bw()"},{"path":"tree.html","id":"tree","chapter":"4 Tree","heading":"4 Tree","text":"","code":""},{"path":"random-forest-1.html","id":"random-forest-1","chapter":"5 Random Forest","heading":"5 Random Forest","text":"","code":""},{"path":"xgboost-1.html","id":"xgboost-1","chapter":"6 xgboost","heading":"6 xgboost","text":"","code":""},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"7 Logistic Regression","heading":"7 Logistic Regression","text":"","code":""},{"path":"glmnet-1.html","id":"glmnet-1","chapter":"8 glmnet","heading":"8 glmnet","text":"","code":""},{"path":"random-forest-2.html","id":"random-forest-2","chapter":"9 Random Forest","heading":"9 Random Forest","text":"","code":""},{"path":"xgboost-2.html","id":"xgboost-2","chapter":"10 xgboost","heading":"10 xgboost","text":"","code":""},{"path":"tree-1.html","id":"tree-1","chapter":"11 Tree","heading":"11 Tree","text":"","code":""}]
